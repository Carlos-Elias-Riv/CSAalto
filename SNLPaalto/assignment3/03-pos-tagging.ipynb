{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ba78d64eccbd343a075cd94e5fccbf3",
     "grade": false,
     "grade_id": "cell-e4091b77c5a80b9e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 4: POS-tagging\n",
    "\n",
    "## Released: 30.01.2024\n",
    "## Deadline: 09.02.2024 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c72edee5b7972e2f4327ef4a1cc338e",
     "grade": false,
     "grade_id": "cell-435a89bd8e2ebc98",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "**Part-of-speech tagging** (**POS tagging**) is the process of annotating words in an input sequence with their corresponding part-of-speech labels. Word's **part-of-speech** gives us more information about the word itself and about its neighboring words (nouns are preceded by determiners \"a dog\" and adjectives \"nice dog\"). For instance, we can use POS tags as features for the Named Entitity Recognition task: Proper Nouns like names are usually these entities. Moreover, a word's **part-of-speech** provides us with an understanding of how to pronounce this word: cOntent if it is a noun and contEnt if it is an adjective. This helps in such tasks as speech synthesis.\n",
    "\n",
    "In this assignment we're going to create two systems for assigning POS tags. They are both based on statistics collected from a corpus annotated with POS tags by humans. First, we will be assigning words the most frequent tag it has been seen with. This is a **baseline** we will be trying to beat. Second, we will create a **Hidden Markov Model** **HMM** system and compare its performance to our baseline. The POS tagging algorithm is judged by how **accurate** it is. So we are going to use **accuracy** as the performance metric.\n",
    "\n",
    "For this assignment we will be using [the GUM corpus](https://corpling.uis.georgetown.edu/gum/) annotated with [Universal Dependencies POS tags](https://universaldependencies.org/u/pos/). We recomend to study the tags, to understand what each of them means.\n",
    "\n",
    "* `/coursedata/pos-tagging/tags_vocab.txt` - vocabulary of UD tags sorted in alphabetical order\n",
    "* `/coursedata/pos-tagging/train.txt` - corpus for training (4219 sentences, 12181 tokens)\n",
    "* `/coursedata/pos-tagging/words_vocab.txt` - vocabulary of the training corpus sorted in alphabetical order (all word types in the training corpus)\n",
    "* `/coursedata/pos-tagging/test_words.txt` - unlabelled test corpus (1055 sentences, 5262 tokens)\n",
    "* `/coursedata/pos-tagging/test_tags.txt` - correct tags for test corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfde2e5f90a40e7de9088ac5b8b55684",
     "grade": false,
     "grade_id": "cell-2bc068bd0dc9b30f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "* [Task 1: Reading the data](#task_1)\n",
    "    * [Step 1.1: Read the vocabularies](#subtask_1_1)\n",
    "    * [Step 1.2: Read the train corpus](#subtask_1_2)\n",
    "    * [Step 1.3: Read the test corpus](#subtask_1_3)\n",
    "* [Task 2: Study the data](#task_2)\n",
    "    * [Step 2.1: Collect word-to-tag statistics](#subtask_2_1)\n",
    "    * [Step 2.2: Study words and their possible tags / tags and their possible words](#subtask_2_2)\n",
    "    * [Step 2.3: Collect tag-to-tag transition statistics](#subtask_2_3)\n",
    "    * [Step 2.4: Study tag to tag transitions](#subtask_2_4)\n",
    "* [Task 3: Baseline creation](#task_3)\n",
    "    * [Step 3.1: Create a baseline](#subtask_3_1)\n",
    "    * [Step 3.2: Study the baseline results](#subtask_3_2)\n",
    "* [Task 4: HMM POS-tagger](#task_4)\n",
    "    * [Step 4.1: Create Viterbi algorithm ](#subtask_4_1)\n",
    "        * [Step 4.1.1: First column of V](#subtask_4_1_1)\n",
    "        * [Step 4.1.2: The rest of V and P](#subtask_4_1_2)\n",
    "        * [Step 4.1.3: Traceback the Tag sequence](#subtask_4_1_3)\n",
    "    * [Step 4.2: Compare HMM and the Baseline](#subtask_4_2)\n",
    "* [Checklist before submission](#checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42c93aebec0919b1f5a1ba27daa7590e",
     "grade": false,
     "grade_id": "cell-2eaa03fbfd600e2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 1 <a class=\"anchor\" id=\"task_1\"></a>\n",
    "## Read the data\n",
    "\n",
    "In this assignment we are lucky to get already pre-processed text. However, be careful with pre-processing your text for POS tagging: your pre-processing steps should match the pre-processing of the corpus you're collecting statistics from. For example, *POS* tag (Possessive ending) employed in [Penn Treebank Project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) is only used for tokens *'s*, so you need your tokenizer to separate them first.\n",
    "\n",
    "Hopefully, by now it will be easy for you to read three types of text files.\n",
    "\n",
    "## 1.1 <a class=\"anchor\" id=\"subtask_1_1\"></a>\n",
    "### Read the vocabularies (1 point)\n",
    "\n",
    "The files for word and tag vocabularies contain each vocabulary member on its own line. Write a function to collect these vocabularies as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "527ae34fd80e0b4ef9330507297550fd",
     "grade": false,
     "grade_id": "cell-7cac9a2a5bf359fe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_vocab(file_name):\n",
    "    \"\"\" Reads a vocabulary from a .txt file into a list of strings\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a vocabulary file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    vocab : a list of strings\n",
    "        a list of vocabulary words from a .txt file\n",
    "        the elements of the vocabulary should have the same order as in the file\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d655c12a46bb602e2e095c96e9aa1828",
     "grade": true,
     "grade_id": "cell-b12c70ac2a3de7c2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_vocab_path = \"/coursedata/pos-tagging/dummy_vocab.txt\"\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(read_vocab(dummy_vocab_path)), list)\n",
    "# check that it's a list of strings\n",
    "assert_equal(type(read_vocab(dummy_vocab_path)[0]), str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "correct_dummy_vocab = ['this', 'is', 'a', 'dummy', 'voabulary', '!']\n",
    "assert_equal(read_vocab(dummy_vocab_path), correct_dummy_vocab)\n",
    "\n",
    "# SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "# check that the length is right\n",
    "assert_equal(len(read_vocab('/coursedata/pos-tagging/tags_vocab.txt')), 17)\n",
    "assert_equal(len(read_vocab('/coursedata/pos-tagging/words_vocab.txt')), 12181)\n",
    "# check that the order is correct\n",
    "assert_equal(read_vocab('/coursedata/pos-tagging/tags_vocab.txt')[12], 'PUNCT')\n",
    "assert_equal(read_vocab('/coursedata/pos-tagging/words_vocab.txt')[303], '213')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "819774628bb6c4f9fd90a8ae51173d92",
     "grade": false,
     "grade_id": "cell-73fe73e09cff48f2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.2 <a class=\"anchor\" id=\"subtask_1_2\"></a>\n",
    "### Read the training corpus (1 point)\n",
    "\n",
    "The training corpus contains 4219 sentences with words labeled with their correct POS tags by humans. Each sentence is located on its own line, the words are separated from each other by whitespaces. The word is separated from its tag like this: Word_/_TAG. \n",
    "\n",
    "Your task is to write a function that creates a list of sentences. Each sentence is, in turn, a list of tuples, where the first element is a word and the second element is its tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53e3626a0bf08f5b41489f5e3d9f24bd",
     "grade": false,
     "grade_id": "cell-a8126a1c6db8ff3a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_train(file_name):\n",
    "    \"\"\" Read the training corpus of POS-tagging\n",
    "    \n",
    "    this function takes in a path to a training corpus file, reads the file,\n",
    "    and returns a list of sentences. each sentence is, in turn, a list of tuples, \n",
    "    where the first element is a word and the second element is its tag\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name - string\n",
    "        a path to a file with an annotated corpus\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    words_and_tags - a list of lists of tuples\n",
    "        For example, the first sentence in a file is 'word1_/_tag word2_/_tag' \n",
    "        and the next sentence is 'word3_/_tag'. Then you should get:\n",
    "        [[('word1','tag'),('word2', 'tag')],[('word3','tag')]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return words_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5259526814f2debc7795c5973bde6a51",
     "grade": true,
     "grade_id": "cell-bb5bbeed9756fb81",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_train_path = \"/coursedata/pos-tagging/dummy_train.txt\"\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(read_train(dummy_train_path)), list)\n",
    "# check that it's a list of lists\n",
    "assert_equal(type(read_train(dummy_train_path)[0]), list)\n",
    "# check that it's a list of lists of tuples\n",
    "assert_equal(type(read_train(dummy_train_path)[0][0]), tuple)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "correct_dummy_train = [[('word1', 'TAG1'), ('word2', 'TAG2')],\n",
    "                       [('word3', 'TAG3'), ('word4', 'TAG4')]]\n",
    "\n",
    "assert_equal(read_train(dummy_train_path), correct_dummy_train)\n",
    "\n",
    "# SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "assert_equal(len(read_train(\"/coursedata/pos-tagging/train.txt\")), 4219)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5b8ae57f1b611f5a72af1ccd34a964e",
     "grade": false,
     "grade_id": "cell-0b10622ba9f943e8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.3 <a class=\"anchor\" id=\"subtask_1_2\"></a>\n",
    "### Read the test corpus (1 point)\n",
    "The test corpus is located in two separate files. One contains 1055 unlabeled test sentences. Each sentence is located on its own line. Another file contains corresponding 1055 sequences of tags. Each tag sequence is located on its own line. Both words and tags are separated from each other by whitespaces.\n",
    "\n",
    "The purpose of the separation is to compare how our systems differ from the original tag sequene, and what system is closer to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbfc97d136ebef30d0b3cc90af5561dd",
     "grade": false,
     "grade_id": "cell-8e0395f8c4a35597",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_test(file_name):\n",
    "    \"\"\" Reads test subcorpus\n",
    "    \n",
    "    this function takes in a path to a test corpus file, reads the file,\n",
    "    and returns it as a list of sentences. each sentence is a list of words or a list of tags.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : string\n",
    "        a path to a part of a test corpus\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    test_sents : a list of lists\n",
    "        lists of sentences of tags or words\n",
    "        for example, if a first sentence is 'A B', \n",
    "        and the second sentence is 'C', then you get:\n",
    "        [['A','B'],['C']]\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return test_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cbd93ea73640729e2d6b916a958f91c",
     "grade": true,
     "grade_id": "cell-e9f7ba0d5567131a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_test_path = \"/coursedata/pos-tagging/dummy_test.txt\"\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(read_test(dummy_test_path)), list)\n",
    "# check that it's a list of lists\n",
    "assert_equal(type(read_test(dummy_test_path)[0]), list)\n",
    "# check that it's a list of lists of strings\n",
    "assert_equal(type(read_test(dummy_test_path)[0][0]), str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "correct_dummy_test = [['A', 'B', 'C'], ['D', 'E', 'F', 'G']]\n",
    "\n",
    "assert_equal(read_test(dummy_test_path), correct_dummy_test)\n",
    "\n",
    "# SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "# check that the number of sentences is right\n",
    "assert_equal(len(read_test(\"/coursedata/pos-tagging/test_words.txt\")), 1055)\n",
    "assert_equal(len(read_test(\"/coursedata/pos-tagging/test_tags.txt\")), 1055)\n",
    "# check that the length of a first sentence is correct\n",
    "assert_equal(len(read_test(\"/coursedata/pos-tagging/test_words.txt\")[0]), 19)\n",
    "assert_equal(len(read_test(\"/coursedata/pos-tagging/test_tags.txt\")[0]), 19)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53b4adfb18c1c50bab700fe0242af933",
     "grade": false,
     "grade_id": "cell-7fd18fccf9c1b3f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Load the data\n",
    "Load all of our data parts by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9652d48b02a5debff2ccb7d0e23e2585",
     "grade": false,
     "grade_id": "cell-ef97ebcb2ad843f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the list of tag types\n",
    "tags_vocab = read_vocab(\"/coursedata/pos-tagging/tags_vocab.txt\")\n",
    "# the list of all the word types present in the train corpus\n",
    "words_vocab = read_vocab(\"/coursedata/pos-tagging/words_vocab.txt\")\n",
    "\n",
    "# the train corpus\n",
    "words_and_tags = read_train('/coursedata/pos-tagging/train.txt')\n",
    "\n",
    "# the sentences of test set with words as their tokens\n",
    "test_words = read_test(\"/coursedata/pos-tagging/test_words.txt\")\n",
    "# the sentences of test set with tags as their tokens\n",
    "test_tags = read_test(\"/coursedata/pos-tagging/test_tags.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebcc8c7c607707230d5158af9657d1b5",
     "grade": false,
     "grade_id": "cell-342c25e00e2169b1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 2 <a class=\"anchor\" id=\"task_2\"></a>\n",
    "## Study the data\n",
    "\n",
    "It is always good to look closely at your data. In this task we're going to study things like: how many word types are ambiguous (can have more than one tag), what the most popular POS in English is, what POS is most likely to start a sentence...\n",
    "\n",
    "## 2.1 <a class=\"anchor\" id=\"subtask_2_1\"></a>\n",
    "### Collect word to tag statistics (1 point)\n",
    "Here, we're going to create a word-tag co-occurance matrix. This matrix is a way of storing the information about what POS tags were assigned to words and how many times a word had a particular tag.\n",
    "\n",
    "Using our training corpus, create a matrix, where rows are words and columns are tags. The cells of this matrix are the number of times a word was seen with some particular tag.\n",
    "\n",
    "For example, imagine that our training corpus looks like this:\n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/corpus.png\">\n",
    "\n",
    "Then, our word to tag matrix will be as follows:\n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/wt.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c9e2ab444e4375c87d22837934ada03",
     "grade": false,
     "grade_id": "cell-ad6d2f7baff45b19",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_word_tag_matrix(training_corpus, vocab_words, vocab_tags):\n",
    "    \"\"\" Creates word-tag co-occurance matrix\n",
    "    \n",
    "    this function takes in a training_corpus, its word and tag vocabularies,\n",
    "    and creates a word-tag count matrix. if some word has never been seen together with a tag,\n",
    "    their cell should be zero. the order of rows and columns should stay as given in vocabularies.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_corpus : a list of lists\n",
    "        training corpus as list of sentences with tuples of words and their tags\n",
    "        [[('word1','tag'),('word2', 'tag')],[('word3','tag')]]\n",
    "    vocab_words : a list of strings\n",
    "        a list of word types in the training corpus\n",
    "    vocab_tags : \n",
    "        a list of possible UD tag labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    wt_matrix - a numpy array \n",
    "        a matrix containg word to tag counts \n",
    "        the matrix has shape (len(vocab_words) X len(vocab_tags))\n",
    "    \"\"\"\n",
    "    \n",
    "    wt_matrix = np.zeros((len(vocab_words), len(vocab_tags)))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return wt_matrix.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d79a30cf637d2b4c362044bf572cbca4",
     "grade": true,
     "grade_id": "cell-d7227c042c0c4dfc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "dummy_train = [[('word1', 'TAG1'), ('word2', 'TAG2')],\n",
    "               [('word3', 'TAG2'), ('word4', 'TAG2'), ('word2', 'TAG1')]]\n",
    "dummy_word_vocabulary = ['word1','word2','word3','word4']\n",
    "dummy_tag_vocabulary = ['TAG1','TAG2']\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(create_word_tag_matrix(dummy_train, \n",
    "                                          dummy_word_vocabulary,\n",
    "                                          dummy_tag_vocabulary).shape, (4,2))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_wt_dummy_matrix = np.array([[1., 0.],\n",
    "                                    [1., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [0., 1.]])\n",
    "                                   \n",
    "assert_array_equal(create_word_tag_matrix(dummy_train, \n",
    "                                          dummy_word_vocabulary,\n",
    "                                          dummy_tag_vocabulary), correct_wt_dummy_matrix)\n",
    "\n",
    "\n",
    "# SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "# checking the shape\n",
    "assert_equal(create_word_tag_matrix(words_and_tags, \n",
    "                                    words_vocab,\n",
    "                                    tags_vocab).shape, (12181, 17))\n",
    "# checkingt he contents\n",
    "\n",
    "\n",
    "assert_equal(create_word_tag_matrix(words_and_tags, \n",
    "                                    words_vocab,\n",
    "                                    tags_vocab)[0][12], 75)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c2298ba254de9576ca4ef93bf5fe018",
     "grade": false,
     "grade_id": "cell-2c0a2778829299c4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### create the word-tag matrix\n",
    "Run the cell below to collect the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa0d4e1183194a76896c212f6c74f709",
     "grade": false,
     "grade_id": "cell-0803a70e321906e9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wt_matrix = create_word_tag_matrix(words_and_tags, words_vocab, tags_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2387129bcb91fd2f448de13e80178657",
     "grade": false,
     "grade_id": "cell-310ab36e57c271ee",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.2  <a class=\"anchor\" id=\"subtask_2_2\"></a>\n",
    "###  Study words and their possible tags / tags and their possible words (3 points)\n",
    "Word classes (POS) differ from each other. Some are very easy to label because they don't have a lot of words in them. For example, pronouns ('me','you','he'...). Classes like this are called 'closed' because there are no new words coming into them. Classes like nouns are called 'open' because they contain an unlimited amount of words and new words are coming into them all the time. Some words might belong to several classes simulateously. For example, 'close' can be a verb as in 'close the door' or an adjective as in 'my close friend'. Words like this are called 'ambigous' because we're not sure what class out of several possible to assign them to.\n",
    "\n",
    "Looking at the matrix we've created, answer the following questions:\n",
    "1. What is the most frequent tag (a tag that was assigned the most amount of times)?\n",
    "2. What tag was given to the smallest number of different word TYPES? (i.e. what is the most 'closed' POS)\n",
    "3. What is the maximum number of different tag TYPES one word TYPE in the training corpus has? \n",
    "4. What is the word type with the maximum number of different tag TYPES? (i.e. the most 'ambigous' word)\n",
    "5. How many word TYPES are ambiguous (words having more than only one tag TYPE)?\n",
    "6. What is the proportion of unambiguous word TYPES in the vocabulary (how many word types out all word types have only one tag in the train corpus)?\n",
    "\n",
    "You can create a new cell to do the calculations if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cbff8c4e3ee5adc89e41f98edd09fbd",
     "grade": false,
     "grade_id": "cell-ad2ee637a8c861e6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 1\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# most_popular_tag = \"ADJ\"\n",
    "most_popular_tag = None\n",
    "\n",
    "# QUESTION 2\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# least_open_tag = \"ADJ\"\n",
    "least_open_tag = None\n",
    "\n",
    "# QUESTION 3\n",
    "# type in the answer as an integer.\n",
    "# For example:\n",
    "# max_n_of_different_tags = 2\n",
    "max_n_of_different_tags = None\n",
    "\n",
    "# QUESTION 4\n",
    "# type in the answer as a string. it should be written exactly as in the word vocabulary.\n",
    "# For example:\n",
    "# most_ambiguos_word = \".\"\n",
    "most_ambiguos_word = None\n",
    "\n",
    "# QUESTION 5\n",
    "# type in the answer as an integer.\n",
    "# For example:\n",
    "# n_of_unambiguous_words = 200\n",
    "n_of_ambiguous_words = None\n",
    "\n",
    "# QUESTION 6\n",
    "# type in the answer as a float number from 0 to 1.\n",
    "# For example:\n",
    "# part_of_unambiguous_words = 0.2\n",
    "part_of_unambiguous_words_in_vocab = None\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c7fab7b81d554211813cd18fad99475",
     "grade": true,
     "grade_id": "cell-64c2794a48a2c36a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 1 tests\n",
    "\n",
    "# checks if your answer is a string\n",
    "assert_equal(type(most_popular_tag), str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e9b60c774892bf6299af0d6b4bdc860",
     "grade": true,
     "grade_id": "cell-aafab755c600dd62",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks if your answer is a string\n",
    "assert_equal(type(least_open_tag), str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5beedc121be373474a47b8c84b34c28f",
     "grade": true,
     "grade_id": "cell-461db1ec42493447",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 3 tests\n",
    "\n",
    "# checks if your answer is an int\n",
    "assert_equal(type(max_n_of_different_tags), int)\n",
    "# check that it is remotely correct\n",
    "assert(max_n_of_different_tags in range(3,11))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52be509fe1a4bd985092e0895f45a171",
     "grade": true,
     "grade_id": "cell-157f482ed307bfba",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 4 tests\n",
    "\n",
    "# checks if your answer is a string\n",
    "assert_equal(type(most_ambiguos_word), str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b46afdbb08fd9c5b1b2863c6c34f7e9",
     "grade": true,
     "grade_id": "cell-be05f09fb8bdd6c7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 5 tests\n",
    "\n",
    "# checks if your answer is an int\n",
    "assert_equal(type(n_of_ambiguous_words), int)\n",
    "# checks if it is remotely correct\n",
    "assert(n_of_ambiguous_words in range (800,1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da64d3228ed690641a7ea79d530c73d8",
     "grade": true,
     "grade_id": "cell-da49671111b90ef0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 6 tests\n",
    "\n",
    "# checks if your answer is a float\n",
    "assert_equal(type(part_of_unambiguous_words_in_vocab), float)\n",
    "# checks if it is remotely correct\n",
    "assert(part_of_unambiguous_words_in_vocab >= 0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc25a5198308ae577314a0ba3522dc2b",
     "grade": false,
     "grade_id": "cell-003d0b98175c7849",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.3  <a class=\"anchor\" id=\"subtask_2_3\"></a>\n",
    "### Collect tag-to-tag transition statistics (3 points)\n",
    "\n",
    "Another thing we can easily do with our data is creating a bi-gram language model for tags! We will represent it as a tag-to-tag transition matrix. We're going to collect the number of times one POS tag is followed by another POS tag (transitions from one tag to another).\n",
    "\n",
    "As you remember from our language modelling assignment, we also want the information about what tag starts a sentence and what tag ends it, so we will need to modify our tag sequences by appending special start-of-sentence **&lt;s>** and end-of-sentence **&lt;/s>** symbols.\n",
    "\n",
    "Create a tag-to-tag transition matrix to capture this information. **The first row** of the matrix will correspond to the start symbol, other rows are just tags in their alphabetical order. The columns of the matrix are, again, tags in their alphabetical order, and **the last column** is an end of sentence tag. Each cell corresponds to the number of times a column tag was seen after a row tag in our training corpus.\n",
    "\n",
    "For our toy corpus this matrix will look this way:\n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/tt.png\">\n",
    "\n",
    "We can see, for example, that only NOUNS can end sentences according to out toy corous. Here is the corpus again, so that it is easy to check that the tag-to-tag matrix makes sense:\n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/corpus.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fcc82d4002642917a61405b25bd8d3a",
     "grade": false,
     "grade_id": "cell-0963a4980b066610",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tag_to_tag_transition_matrix(training_corpus, vocab_tags):\n",
    "    \"\"\" Creates tag-to-tag transition matrix\n",
    "    \n",
    "    This function takes in a training corpus and its tag vocabulary,\n",
    "    and returns a tag_to_tag_transition_matrix of size [len(tag_vocabulary)+1 X len(tag_vocabulary)+1]\n",
    "    the cells contain the counts of how many times a row tag was followed by a column tag\n",
    "    the first row contains the number of times each tag started a sentence\n",
    "    the last column contains the number of times each tag ended a sentence\n",
    "    tags should be in the same order as they were in the vocabulary file\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_corpus : a list of lists\n",
    "        [[('word1','tag'),('word2', 'tag')],[('word3','tag')]]\n",
    "    vocab_tags - a list of strings \n",
    "        UD tags\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tag_transition_matrix : an numpy array of shape [len(tag_vocabulary)+1 X len(tag_vocabulary)+1]\n",
    "        contains tag to tag transition statistics\n",
    "    \"\"\"\n",
    "    tag_transition_matrix = np.zeros((len(vocab_tags)+1, len(vocab_tags)+1))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return tag_transition_matrix.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98d4a1a5bdf918ad305a9f008ecf6db2",
     "grade": true,
     "grade_id": "cell-dc35a6a959191ea0",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_train2 = [[('word1', 'TAG1'), ('word2', 'TAG2')],\n",
    "               [('word3', 'TAG1'), ('word4', 'TAG2'), ('word2', 'TAG1')]]\n",
    "\n",
    "dummy_tag_vocabulary = ['TAG1','TAG2']\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(create_tag_to_tag_transition_matrix(dummy_train2, dummy_tag_vocabulary).shape, (3,3))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_tt_dummy_matrix = np.array([[2, 0, 0],\n",
    "                                    [0, 2, 1],\n",
    "                                    [1, 0, 1]])\n",
    "                                   \n",
    "assert_array_equal(create_tag_to_tag_transition_matrix(dummy_train2,dummy_tag_vocabulary), correct_tt_dummy_matrix)\n",
    "\n",
    "\n",
    "# SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "# check the shape\n",
    "assert_equal(create_tag_to_tag_transition_matrix(words_and_tags, tags_vocab).shape, (18,18))\n",
    "# check the contents\n",
    "assert_equal(create_tag_to_tag_transition_matrix(words_and_tags, tags_vocab)[0][0], 153)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c58a2dcebdd3ae29e7583fa2091521d7",
     "grade": false,
     "grade_id": "cell-52406232eadfe4f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### create and display the tag-to-tag transition matrix \n",
    "run the cell below to collect the transition counts and to display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0acd39a3c39ac3a7f2326e7c986a6212",
     "grade": false,
     "grade_id": "cell-31f2b496b8cdd8b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tt_matrix = create_tag_to_tag_transition_matrix(words_and_tags, tags_vocab)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data=tt_matrix, index=[\"START\"]+tags_vocab, columns=tags_vocab+[\"END\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bed916c18e0bd3296965f4e11ff5f713",
     "grade": false,
     "grade_id": "cell-a411d2a2dae081a0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.4 <a class=\"anchor\" id=\"subtask_2_4\"></a>\n",
    "###  Study tag-to-tag transitions (3 points)\n",
    "\n",
    "Looking at the tag-to-tag transition matrix we've created, answer the following question:\n",
    "1. What is the most popular tag bi-gram? \n",
    "2. What tag is most likely to follow the adjective tag?\n",
    "3. What tag is most likely to precede an interjection?\n",
    "4. What tag is most likely to start a sentence?\n",
    "5. What tag is most likely to end a sentence?\n",
    "6. How many tags can never end a sentence according to our training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef6ab311d48803129135083f626964db",
     "grade": false,
     "grade_id": "cell-4dfa044a9f4fdaa4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#QUESTION 1\n",
    "# type in the answer as a tuple containing two strings. write tags exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# most_popular_tag_bi_gram = ('ADJ', 'ADJ')\n",
    "most_popular_tag_bi_gram = None\n",
    "\n",
    "#QUESTION 2\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# tag_after_adj = 'ADJ'\n",
    "tag_after_adj = None\n",
    "\n",
    "#QUESTION 3\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# tag_before_intj = 'ADJ'\n",
    "tag_before_intj = None\n",
    "\n",
    "#QUESTION 4\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# start_tag = 'ADJ'\n",
    "start_tag = None\n",
    "\n",
    "#QUESTION 5\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# end_tag = 'ADJ'\n",
    "end_tag = None\n",
    "\n",
    "#QUESTION 6\n",
    "# type in the answer as an integer number.\n",
    "# For example:\n",
    "# n_of_non_final_tags = 15\n",
    "n_of_non_final_tags = None\n",
    "\n",
    "\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1dbd30e2702f07bd988aab167181cb45",
     "grade": true,
     "grade_id": "cell-7d22ada9fa0ff180",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 1 tests\n",
    "\n",
    "# checks if your answer is a tuple of strings\n",
    "assert_equal(type(most_popular_tag_bi_gram), tuple)\n",
    "assert_equal(type(most_popular_tag_bi_gram[0]), str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34c1f2a8869b6d154ffd7d5ca810030b",
     "grade": true,
     "grade_id": "cell-396ee42898e6087f",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks if your answer is a string\n",
    "assert_equal(type(tag_after_adj), str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b587055132135fc11b3635271400e908",
     "grade": true,
     "grade_id": "cell-a0355e2467748b49",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 3 tests\n",
    "\n",
    "# checks if your answer is a string\n",
    "assert_equal(type(tag_before_intj), str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "810b35d2cc5787599e7f4154a178e785",
     "grade": true,
     "grade_id": "cell-a241f37ff9f8c966",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 4 tests\n",
    "\n",
    "# checks if your answer is a string\n",
    "assert_equal(type(start_tag), str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f52f2a69dbfd44abcd340a9888d485d8",
     "grade": true,
     "grade_id": "cell-62e75a1cab442bee",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 5 tests\n",
    "\n",
    "# checks if your answer is a string\n",
    "assert_equal(type(end_tag), str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4f804ffaa7ad264d9fb8bfd75d7e3d5",
     "grade": true,
     "grade_id": "cell-e3c2b5640489fb6e",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 6 tests\n",
    "\n",
    "# checks if your answer is an int\n",
    "assert_equal(type(n_of_non_final_tags), int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eca1c04d36aa944fa1451bd724aca2a",
     "grade": false,
     "grade_id": "cell-deccc5f1e168b2fb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 3 <a class=\"anchor\" id=\"task_3\"></a>\n",
    "## Baseline creation\n",
    "\n",
    "## 3.1 <a class=\"anchor\" id=\"subtask_3_1\"></a>\n",
    "### Create a baseline (1 point)\n",
    "\n",
    "\n",
    "To understand if our tagger is any good we will need to compare it to some baseline model. One popular approach is to assign each word its most frequent tag. Meaning a tag that this word was labeled with the most times in our training data.\n",
    "\n",
    "Create a function that labels word sequences with the most frequent tags for these words. If some word has several tags with the same frequency, just **select the one that comes first alphabetically. Assign the words unseen in the training corpus with the 'X' tag.** <span style=\"color: red;\"> Note that 'X' tag is not something we've just come up with to mark words unseen at test time. 'X' tag is used in the training corpus and in the test corpus! There are words in both the training corpus and the test corpus marked with this tag! You can read about the use of the tag [here](https://universaldependencies.org/u/pos/X.html). We could have just as well agreed on marking unseen words with NOUN or SYM. Understanding this part will help you answering questions in 3.2 correctly!!!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "228f417113246a8ff85cef5b0e25dbf8",
     "grade": false,
     "grade_id": "cell-b7d6237171831faa",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def baseline(word_tag_matrix , test_sents, vocab_words, vocab_tags):\n",
    "    \"\"\" Tags words in a test corpus with the most frequent tag\n",
    "    \n",
    "    this function takes in word to tag matrix, test sentences to label, word and tag vocabularies, \n",
    "    and assigns every word in test sentences the most frequent tag it was seen with in the training corpus.\n",
    "    mark a word with \"X\" tag if it was absent in the training corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word_tag_matrix : a numpy array \n",
    "        contains word to tag statistics [len(vocab_words) X len(vocab_tags)]\n",
    "    test_sents : a list of lists\n",
    "        [['word1','word2'],['word3']]\n",
    "    vocab_words : a list of strings\n",
    "        word types in the training corpus\n",
    "    vocab_tags : a list of strings\n",
    "        a list of UD tag types\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    test_tags_predicted : a list of lists\n",
    "        predicted tags for words in test_sents.\n",
    "        [['tag1','tag2'],['tag3']]\n",
    "    \"\"\"\n",
    "    most_freq_tag = {}\n",
    "    for ind, word in enumerate(vocab_words):\n",
    "        tag_ind = np.argmax(word_tag_matrix[ind])\n",
    "        most_freq_tag[word] = vocab_tags[tag_ind]\n",
    "        \n",
    "    test_tags_predicted = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return test_tags_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc5a72a193029768909959a736455c0",
     "grade": true,
     "grade_id": "cell-af1a8d45182cf117",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "wt_dummy_train = np.array([[1., 0.],\n",
    "                           [1., 1.],\n",
    "                           [1., 0.],\n",
    "                           [0., 1.]])\n",
    "\n",
    "dummy_word_vocabulary = ['word1','word2','word3','word4']\n",
    "dummy_tag_vocabulary = ['TAG1','TAG2']\n",
    "dummy_test = [['word1','word2','word5']]\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check that the output is a list\n",
    "assert_equal(type(baseline(wt_dummy_train, dummy_test, dummy_word_vocabulary, dummy_tag_vocabulary)),list)\n",
    "# check that the output is a list of lists\n",
    "assert_equal(type(baseline(wt_dummy_train, dummy_test, dummy_word_vocabulary, dummy_tag_vocabulary)[0]),list)\n",
    "# check that the output is a list of lists of strings\n",
    "assert_equal(type(baseline(wt_dummy_train, dummy_test, dummy_word_vocabulary, dummy_tag_vocabulary)[0][0]),str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_dummy_tags = [['TAG1', 'TAG1', 'X']]\n",
    "                                   \n",
    "assert_equal(baseline(wt_dummy_train, dummy_test, dummy_word_vocabulary, dummy_tag_vocabulary),\n",
    "             correct_dummy_tags)\n",
    "\n",
    "# SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "assert_equal(len(baseline(wt_matrix, test_words, words_vocab, tags_vocab)), 1055)\n",
    "assert_equal(baseline(wt_matrix, test_words, words_vocab, tags_vocab)[0][0], 'PROPN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "981d09e98e30f0ad4b9c872c37a6afeb",
     "grade": false,
     "grade_id": "cell-acefdf7555be300d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### The baseline performance\n",
    "First, we're going to evaluate the baseline according to its **accuracy**. We're interested in the **percentage** of words in the test set that were assigned their correct tags.\n",
    "\n",
    "Run the cell below to estimate the accuracy of our baseline. It should be around 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7f1ec511b8978c9031416fea99c43c6",
     "grade": false,
     "grade_id": "cell-653225defb98c511",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tags_predicted = baseline(wt_matrix, test_words, words_vocab, tags_vocab)\n",
    "print(accuracy_score([tag for sentence in test_tags for tag in sentence],\n",
    "                     [tag for sentence in tags_predicted for tag in sentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed449011a1043e0c6a7a02a1d1cc74f7",
     "grade": false,
     "grade_id": "cell-806c302465c18e6f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Confusion matrix\n",
    "\n",
    "The accuracy score of your baseline model should be around 85 percent. But it doesn't tell us much about what's going wrong and right.\n",
    "\n",
    "We will create a confusion matrix to look deeper into the results. A confusion matrix tells us how many times each true tag was predicted as itself and as some other tag. The rows of the matrix are correct labels, the columns are all tags it could have been confused with. A cell tells how many times a true tag was predicted as some column tag.\n",
    "\n",
    "For example, we have the following sequences of tags:\n",
    "* `y_true = ['PRON', 'ADJ', 'ADJ', 'NOUN']`\n",
    "* `y_predicted = ['NOUN', 'ADJ', 'NOUN', 'NOUN']`\n",
    "\n",
    "The sequence of tags that labels rows and columns is `['ADJ', 'NOUN', 'PRON']`, then our confusion matrix will look like this:\n",
    "\n",
    "` \n",
    "1 1 0 \n",
    "0 1 0 \n",
    "0 1 0\n",
    "`\n",
    "\n",
    "* The element `[0][0] = 1` tells us that `'ADJ'` was correctly predicted as itself once.\n",
    "* The element `[0][1] = 1` tells us that `'ADJ'` was confused with `'NOUN'` once.\n",
    "* The element `[1][1] = 1` tells us that `'NOUN'` was correctly predicted as itself once.\n",
    "* The element `[2][1] = 1` tells us that `'PRON'` was confused with `'NOUN'` once.\n",
    "\n",
    "But let's be honest, raw counts are hard to compare and plot. We will need to normalize our matrix: to make a number of predictions of every tag sum to 1. This will help us visually compare mistakes made for both frequent and infrequent tags. After normalization, the true labels in our confusion matrix are rows, the cells in these rows correspond to the fraction of times this tag was predicted as a tag that marks the column. So, for our example it will look like this:\n",
    "\n",
    "`\n",
    "0.5 0.5 0\n",
    " 0   1  0\n",
    " 0   1  0\n",
    "`\n",
    "* The element `[0][0] = 0.5` tells us that `'ADJ'` was correctly predicted as itself only half of the time.\n",
    "* The element `[0][1] = 0.5` tells us that `'ADJ'` was confused with `'NOUN'` half of the time.\n",
    "* The element `[1][1] = 1` tells us that `'NOUN'` was correctly predicted as itself all the time.\n",
    "* The element `[2][1] = 1` tells us that `'PRON'` was confused with `'NOUN'` all the time.\n",
    "\n",
    "Run the cell below to calculate the confusion matrix for our baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebac48b6be9ae301b5ecd2a13e368c59",
     "grade": false,
     "grade_id": "cell-3d49377804262b1b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def create_confusion_matrix(y_true, y_predicted): \n",
    "    \n",
    "    y_true_as_array_of_tags = [tag for sent in y_true for tag in sent]\n",
    "    y_predicted_as_array_of_tags = [tag for sent in y_predicted for tag in sent]\n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(y_true_as_array_of_tags, y_predicted_as_array_of_tags)\n",
    "    \n",
    "    summed_values = cm.sum(axis=1) # sum rows\n",
    "    summed_values = summed_values[:, np.newaxis]\n",
    "    normalized_matrix = cm/summed_values\n",
    "    \n",
    "    return normalized_matrix\n",
    "   \n",
    "cm = create_confusion_matrix(test_tags, tags_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c060ceb657e93becb5c193ef5737b3c",
     "grade": false,
     "grade_id": "cell-a231a19b1a47c8cf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Visualize normalized confusion matrix \n",
    "Run the cell below to plot the confusion matrix for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f2407dcf9924307412acb2e1529255e",
     "grade": false,
     "grade_id": "cell-13752418c5a6213e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, vocab_tags):\n",
    "    \n",
    "    plt.figure(figsize = (9,6))\n",
    "    colors = sns.light_palette((220, 50, 20), input=\"husl\", n_colors=80)\n",
    "    ax = sns.heatmap(np.around(cm, 2),\n",
    "                    annot=True,\n",
    "                    linewidths=.8, \n",
    "                    cmap=colors)\n",
    "    ax.set_ylim(bottom = 17, top=0)\n",
    "    ax.set(xticklabels=vocab_tags)\n",
    "    ax.set(yticklabels=vocab_tags)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=90) \n",
    "    plt.ylabel(\"True Tags\")\n",
    "    plt.xlabel(\"Predicted Tags\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_confusion_matrix(cm, tags_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42aa9e26b64c720386d16657e45f39a2",
     "grade": false,
     "grade_id": "cell-774600f9bc6d75f0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3.2 <a class=\"anchor\" id=\"subtask_3_2\"></a>\n",
    "### Study the baseline results (3 points)\n",
    "By looking at the visualization of normalized confusion matrix, briefly answer the following questions in the cell below:\n",
    "1. What tags were predicted best? (0.5 points) How would you explain it (what is about these tags that made them easy to assign correctly)? (0.5 points)\n",
    "2. Why some tags (nouns, proper nouns, verbs...) were predicted as 'X' that often? (0.5 points) Why 'X' was rarely mistaken with some other classes? (0.5 points) **Remember that 'X' tag is not solely used for unseen words (as we decided to use it) but has an actual role in both training and test corpus already.**\n",
    "3. How many unseen words were in the test corpus? (0.5 points) Does it affect the performance of the baseline algorithm a lot? How much? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24066a8f08ee8a84067843af04e80521",
     "grade": true,
     "grade_id": "cell-e380c308eb26462f",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3760b966189dead6e289c5ce3985f0e",
     "grade": false,
     "grade_id": "cell-92be2872dd47c4f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 4 <a class=\"anchor\" id=\"task_4\"></a>\n",
    "## HMM POS-tagger\n",
    "An **HMM** is a probabilistic sequence model. In our case, given a sequence of words, it computes a probability distribution over possible sequences of POS tags and chooses the best tag sequence. If you find the explanations in the notebook lacking, look [here](https://web.stanford.edu/~jurafsky/slp3/A.pdf) for more information.\n",
    "\n",
    "The *hidden part* of our HMM are tags, because they are some abstract classes that are not directly observed from text sequences. The observed part of our HMM are words these hidden tags produce (yes, we consider that POS tags emit words). \n",
    "The components of our HMM will be:\n",
    "1. **T** - a set of $N$ POS tags\n",
    "\n",
    "2. **$A$** - a transition probability matrix. Each cell $a_{i,j}$ represents the probability of moving from $tag_i$ to $tag_j$: $P(t_j|t_i)$. We also add the start and end probabilities to this matrix, so it has $N+1$ x $N+1$ dimensions. The **row** $a_1$ represents the probabilities of each tag to start a sentence. The last **column** of transition probability matrix $A$ contains the probability of every tag ending a sentence.\n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/A.png\">\n",
    "3. $B$ - an observation likelihood matrix. Each cell $b_{i,j}$ represents a probability of a $word_i$ being generated out of some $tag_j$: $P(w_i|t_j)$\n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/B.png\">\n",
    "\n",
    "Here is the corpus again, so that it is easy to check that $A$ and $B$ make sense.\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/corpus.png\">\n",
    "\n",
    "### Collect probability matrices A and B\n",
    "\n",
    "Turns out, we already have everything for our HMM! We just need to turn the counts that we've collected previously into maximum likelihood probabilities.\n",
    "\n",
    "You can do it by normalizing our word-tag and tag-tag matrices across the appropriate axis. For the tag-tag matrix, we want **the rows to sum up to 1**. For the word-tag matrix, we want **the columns to sum to one**. \n",
    "\n",
    "Take a second to make sure you understand why. With tag-tag matrix, we want the probability of a tag coming after some other tag. With word-tag matrix we want the probability of a tag emit some word.\n",
    "\n",
    "Run the cell below to get the $A$ and $B$ matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49dbb58a8167397282ac0c62ebdc5d71",
     "grade": false,
     "grade_id": "cell-0478bb71a8f20111",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = tt_matrix/tt_matrix.sum(axis=1)[:, np.newaxis]\n",
    "B = wt_matrix/wt_matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b1e8056625b763d47963a544f193988",
     "grade": false,
     "grade_id": "cell-fdb384eee973ab42",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Display the tag transition matrix A by running the cell below\n",
    "Examine that everything looks as you would expect it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d88b80dda56c661aaeb49be1d52665a",
     "grade": false,
     "grade_id": "cell-f49b67adc0e603ce",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=A, index=[\"START\"]+tags_vocab, columns=tags_vocab+[\"END\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db1e2a0f01a6e8c985806766554e2c74",
     "grade": false,
     "grade_id": "cell-18670b0bf05424fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### HMM decoding\n",
    "\n",
    "The aim of an HMM decoding is to choose the tag sequence $t^n_1$ that will be the most probable given the observation sequence of $n$ words $w^n_1$: $\\underset{t^n_1}{\\arg\\max}P(t^n_1|w^n_1)$\n",
    "\n",
    "Using Bayes' rule, we can very conveniently flip this into: $\\underset{t^n_1}{\\arg\\max}\\frac{P(w^n_1|t^n_1)P(t^n_1)}{P(w^n_1)}$. You can also notice, that we don't need the denominator for maximizing the tag sequence probability. Thus:\n",
    "\n",
    "*best tag sequence* $= \\underset{t^n_1}{\\arg\\max}P(w^n_1|t^n_1)P(t^n_1)$\n",
    "\n",
    "Now we can simplify it even further by assuming:\n",
    "\n",
    "1. the probability of a particular tag depends only on the previous tag\n",
    "2. the probability of an observed word depends only on the tag that produced this word\n",
    "\n",
    "*best tag sequence* $= \\underset{t^n_1}{\\arg\\max}\\displaystyle\\prod_{i=1}^{n}P(w_i|t_i)P(t_i|t_{i-1})$\n",
    "\n",
    "Basically, we want to marry two language processes here: \n",
    "1. We want the word order to be probable (like nouns typycally come after adjectives and before verbs).\n",
    "2. We still want the words to be assigned a tag that they are likely to be seen with.\n",
    "\n",
    "BTW! Lucky us, we've already collected probabilities $P(w_i|t_i)$ in the matrix $B$, and $P(t_i|t_{i-1})$ in the matrix $A$! Now we need to come up with how to ${\\arg\\max}$ those. And here comes Viterbi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbe4aa51286b36b68e314d9422d4297e",
     "grade": false,
     "grade_id": "cell-ff09a55b6136a1dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Viterbi algorithm\n",
    "\n",
    "The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states. In our case, its output is the most probable sequence of POS tags for some word sequence. To run this algorith we need to create two additinal tables to keep track of things (see steps 2-3).  \n",
    "\n",
    "* STEP 1: get a sequence you want to tag:\n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/seq.png\" width=\"200\">\n",
    "\n",
    "* STEP 2: create a path probability matrix $V$ with the shape (number of possibe tags, number of words to tag).\n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/table1.png\" width=\"350\">\n",
    "Each cell $V_{i,j}$ keeps the probability of HMM having a tag $i$ after seeing $j$ words and passing through the most probable tag sequence $t_1,t_2...t_{i-1}$.\n",
    "\n",
    "This most probable path of tags so far is represented as the maximum over all previous tag sequences. The probabilities of $V_{i,j}$ are computed by starting from the most probable of the extensions of the paths that lead to the current cell. It might sound very complicated now, but you will see what's going on in later steps. \n",
    "\n",
    "* STEP 3: create a backpointer table $P$ of shape (number of tags, number of words in a sequence).\n",
    "This table is called the **BACK**pointer because it helps us remember where we came from. It is not for what is happening now, but for the past! A cell $P_{i,j}$ answers a question: \"If i am tagging a word $w_j$ with a tag $t_i$ now, what was the tag for the word $w_{j-1}$?\". For example, if we look at the cell $P_{ADJ,hand}$ and see there 0, it means that the previous word was tagged with ADJ!\n",
    "\n",
    "For this table we can skip the first word, since the tag before it is always the beggining of the sentence. But we need to add the imagined \"END\" word (tagged with dummy \"END\" POS tag), to use the knowledge of what tags are most likely to end a sentence.\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/table2.png\">\n",
    "\n",
    "* STEP 4: start filling in the first column of $V$. Each cell $V_{i,1}$ contains a probability of a tag $i$ being the starting tag of the sentence $P(t_i|start)$ multiplied by the probability of the first word in the sequence being generated by this tag $P(word_1|t_i)$. $V_{i,1} = P(t_j|start)*P(word_1|t_j)$. According to our dummy corpus, $V_{NOUN,red} = P(NOUN|start)*P(red|NOUN)$\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/table1_1.png\">\n",
    "\n",
    "* STEP 5 (**probably the trickiest one**): move on to filling in the second column of $V$. Each cell $V_{i,2}$ contains a product of two probabilities:\n",
    "    1. the maximum from the products of every value in the previous column of $V$ and their transition probability to the tag $i$.  This value keeps the balance between what is likely according to the tag language model and what we have seen before (it can be very likely that ADJ is followed by NOUN, but it was very unlikely that the previous word is indeed ADJ). In the example below, we're trying to estimate the probability of \"right\" being an \"ADJ\" ($V_{0,1}$). We want to know: what would be the most probable path that led to the word \"right\" being tagged with \"ADJ\". Was it 'ADJ' 'red' or was it 'NOUN' 'red'? For that, we need to choose between the probabilities of 'red' being 'NOUN' and 'red' being 'ADJ' multiplied by probabilities of coming to another ADJ from those tags.\n",
    "    <img src= \"../../../coursedata/notebook_illustrations/pos-tagging/max.png\">\n",
    "    2. the probability of the second word in the sequence (word 'right') being generated by the tag $j$ $P(word_2|t_j)$\n",
    "    <img src= \"../../../coursedata/notebook_illustrations/pos-tagging/max_and_wt.png\" width=\"250\">\n",
    "    \n",
    "* STEP 6 (connected to STEP 5): save the backpointer. Write down the index of the most probable tag of the previous word! A backpointer is just an $argmax$ of the values computed in 5.1. It is the index of the maximum from the products of every value in the previous column of $V$ and their transition probability to the tag $j$.  In this example, we want to remember that 'right' as 'ADJ' came from the previous word (\"red\") marked as 'ADJ'. So we set the backpointer  to zero $P_{0,0}=0$. This means that if the second word in a sequence was marked as \"ADJ\", the first word in a sequence was also \"ADJ\".\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/backpointer.png\">\n",
    "* STEP 7: compute the rest of the table $V$ as in STEP 5, don't forget to save backpointers as in STEP 6.\n",
    "* STEP 8: compute the final output probabilities of your tag paths by multiplying the last column of V by the probability of each tag ending a sentence. Note: we don't need to write it down in the table since there is nothing in the future to depend on it. But we will use this for backpointer.\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/final.png\">\n",
    "* STEP 9: get last backpointer by chosing the most probable last tag from STEP 8. You can put it in all the cells of the last column for convenience.\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/last_pointer.png\">\n",
    "* STEP 10: trace back the indices in your backpointer table P, starting with the one that was output by STEP 9. Now you can start backtracing with any cell from the last column (because, duh, they are the same). Look at the tag index that this column holds, and go to the cell with this index in the previous column of the backpointer table. In the example below, we see that the last tag had index 1 (so it was NOUN). We look at the NOUN cell in the previous time step column, and see that it was ADJ before (index zero)... and so on until the start of the sentence.\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/path.png\">\n",
    "* STEP 11: return the most probable tag sequence. For this you just need to reverse your backtraced sequence.\n",
    "<img src= \"../../../coursedata/notebook_illustrations/pos-tagging/tags_predicted.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2aa148bad412804390df38a04a04176f",
     "grade": false,
     "grade_id": "cell-57a24ce05e92abb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4.1 <a class=\"anchor\" id=\"subtask_4_1\"></a>\n",
    "### Create Viterbi algorithm (5 points)\n",
    "\n",
    "You're going to write several functions, that we then chain together. It is easier to do some parts of the algorithm in different functions for debugging purposes.\n",
    "* 1 Fill in the first column of $V$.\n",
    "* 2 Fill in the rest of $V$ and keep backpointers $P$.\n",
    "* 3 Backtrace the most probable tag sequence from $P$.\n",
    "\n",
    "\n",
    "* Note 1: to avoid numerical underflow, use log probabilities\n",
    "* Note 2: log probabilities should be summed (not multiplied as the regular probabilities in our formulas)\n",
    "* Note 3: **when you encounter an unseen word (a word that we didn't have in the training corpus), cheat and don't include the observation probabilities (rely only on the tag probabilities only)**\n",
    "\n",
    "### 4.1.1 <a class=\"anchor\" id=\"subtask_4_1_1\"></a>\n",
    "#### First column of V (1 point)\n",
    "Look at step 4 and complete the `viterbi_first_column()` function. Note that we don't keep any backpointers for the first word, because we know that before the first word we could only have an imaginary word \"START\" with an imaginary tag \"START\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b18ce579b406afa0969a2f02b95faa7b",
     "grade": false,
     "grade_id": "cell-028a3861502d6faf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def viterbi_first_column(A, B, word_sequence, tags_vocab, words_vocab):\n",
    "    \"\"\"\n",
    "    Fills in the first column of a path probability matrix V (Step 4)\n",
    "    \n",
    "    If the first word in a sequence was not seen during training, \n",
    "    use only the pribabilities from A matrix. \n",
    "    \n",
    "    We will transform all probabilities into log with numpy.log()\n",
    "    If something used to have a 0 probability, it shoud have a -inf probability in log domain now.\n",
    "    \n",
    "    \n",
    "    Paramters\n",
    "    --------\n",
    "    A : a numpy array \n",
    "        transition probability matrix for POS tags\n",
    "    B : a numpy array \n",
    "        an observation likelihood matrix\n",
    "    word_sequence : a list of strings\n",
    "        a word sequence to tag\n",
    "    tags_vocab : a list of strings\n",
    "        a list of UD tag labels\n",
    "        in the order they're placed in matrix A\n",
    "    words_vocab : a list of strings\n",
    "        a list of word types seen during training \n",
    "        in the order they're placed in matrix B\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    path_probability_matrix - a numpy array\n",
    "        a path probability matrix with first column filled in,\n",
    "        other columns have zeros in them\n",
    "        the shape of the matrix is len(tags_vocab) X len(word_sequence)\n",
    "    \"\"\"\n",
    "    \n",
    "    # a template for a the V matrix with -111.0 in every cell\n",
    "    # NOTE: -111.0 are just placeholders here\n",
    "    \n",
    "    path_probability_matrix = np.full((len(tags_vocab), len(word_sequence)), -111.)\n",
    "    \n",
    "    # Let's change values in A and B to log\n",
    "    # We're catching warnings of numpy converting zeros\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "        A = np.log(A)\n",
    "        B = np.log(B)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return path_probability_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c75b878fed59c3bc5af74f928de2d196",
     "grade": true,
     "grade_id": "cell-f79366718fb8e67b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_allclose\n",
    "import warnings\n",
    "\n",
    "dummy_A = np.array([[3/4,1/4,0],\n",
    "                    [1/4,3/4,0],\n",
    "                    [0,0,1.]])\n",
    "dummy_B = np.array([[0,1],\n",
    "                    [3/4,0],\n",
    "                    [1/4, 0]])\n",
    "dummy_test = ['red','right','hand']\n",
    "dummy_tags_vocab = [\"ADJ\", \"NOUN\"]\n",
    "dummy_words_vocab = ['hand','red','right']\n",
    "\n",
    "\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check that the output is a numpy array\n",
    "assert_equal(type(viterbi_first_column(dummy_A, \n",
    "                                       dummy_B, \n",
    "                                       dummy_test, \n",
    "                                       dummy_tags_vocab, \n",
    "                                       dummy_words_vocab)),np.ndarray)\n",
    "\n",
    "# check that the output is the right shape\n",
    "assert_equal(viterbi_first_column(dummy_A, \n",
    "                                   dummy_B, \n",
    "                                   dummy_test, \n",
    "                                   dummy_tags_vocab, \n",
    "                                   dummy_words_vocab).shape, (len(dummy_tags_vocab), len(dummy_test)))\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check the function is giving out the right probabilities\n",
    "assert_almost_equal(viterbi_first_column(dummy_A,\n",
    "                                          dummy_B,\n",
    "                                          dummy_test,\n",
    "                                          dummy_tags_vocab,\n",
    "                                          dummy_words_vocab)[0][0], np.log(0.562), 2)\n",
    "\n",
    "\n",
    "assert_almost_equal(viterbi_first_column(dummy_A,\n",
    "                                          dummy_B,\n",
    "                                          dummy_test,\n",
    "                                          dummy_tags_vocab,\n",
    "                                          dummy_words_vocab), np.array([[-0.57536414, -111., -111.],[np.NINF, -111., -111.]]), 2)\n",
    "\n",
    "# checking that the function is working with unknown words\n",
    "dummy_test2 = ['leg','right','red']\n",
    "assert_equal(viterbi_first_column(dummy_A,\n",
    "                                  dummy_B, \n",
    "                                  dummy_test2, \n",
    "                                  dummy_tags_vocab,\n",
    "                                  dummy_words_vocab)[0][0], np.log(dummy_A[0][0]))\n",
    "\n",
    "assert_equal(viterbi_first_column(dummy_A,\n",
    "                                  dummy_B, \n",
    "                                  dummy_test2, \n",
    "                                  dummy_tags_vocab,\n",
    "                                  dummy_words_vocab)[1][0], np.log(dummy_A[0][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf08cae13c70d317abf2bce3f3497ef7",
     "grade": false,
     "grade_id": "cell-f8a4fa43f1cfcc84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.1.2 <a class=\"anchor\" id=\"subtask_4_1_2\"></a>\n",
    "#### The rest of V and P (3 points)\n",
    "Steps 5-9\n",
    "\n",
    "Now fill in the rest of $V$ and the whole $P$.\n",
    "For the last backpointer, just fill in the whole last column of $P$ with the most probable tag for the last word. \n",
    "\n",
    "**NOTE: if at step with max and argmax there are tags with the same probability, choose the one that comes first index-wise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d08e389c35010cb102df75120c7022b3",
     "grade": false,
     "grade_id": "cell-52568698055b74b2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def viterbi_rest(path_probability_matrix, A, B, word_sequence, tags_vocab, words_vocab):\n",
    "    \"\"\" Fills in V and P matrices\n",
    "    \n",
    "     \n",
    "    Paramters\n",
    "    --------\n",
    "    path_probability_matrix: a numpy array\n",
    "        a path probability matrix with the first column filled in\n",
    "        (the result of viterbi_first_column())\n",
    "    A : a numpy array \n",
    "        transition probability matrix for POS tags\n",
    "    B : a numpy array \n",
    "        an observation likelihood matrix\n",
    "    word_sequence : a list of strings\n",
    "        a word sequence to tag\n",
    "    tags_vocab : a list of strings\n",
    "        a list of UD tag labels\n",
    "        in the order they're placed in matrix A\n",
    "    words_vocab : a list of strings\n",
    "        a list of word types seen during training \n",
    "        in the order they're placed in matrix B\n",
    "      \n",
    "    Returns\n",
    "    -------\n",
    "    path_probability_matrix : a numpy array\n",
    "        matrix V\n",
    "    backpointer_table : a numpy array\n",
    "        a matrix P with backpointers\n",
    "    \"\"\"\n",
    "    \n",
    "    # a template for a backpointer table with -111 in every cell as placeholders\n",
    "    backpointer_table = np.full((len(tags_vocab), len(word_sequence)), -111)\n",
    "    \n",
    "    # Let's change values in A and B to log\n",
    "    # We're catching warnings of numpy converting zeros\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "        A = np.log(A)\n",
    "        B = np.log(B)\n",
    "    \n",
    "    \n",
    "    # FILL IN THE REST OF THE path_probability_matrix\n",
    "    # DON'T FORGET TO KEEP BACKPOINTERS\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return path_probability_matrix, backpointer_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c2de30f929766385db2a5cbcaa20541",
     "grade": true,
     "grade_id": "cell-3e8d38c3c1946db3",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_A = np.array([[3/4,1/4,0],\n",
    "                    [1/4,3/4,0],\n",
    "                    [0,0,1.]])\n",
    "\n",
    "dummy_B = np.array([[0,1],\n",
    "                    [3/4,0],\n",
    "                    [1/4, 0]])\n",
    "\n",
    "dummy_test = ['red','right','hand']\n",
    "dummy_tags_vocab = [\"ADJ\", \"NOUN\"]\n",
    "dummy_words_vocab = ['hand','red','right']\n",
    "dummy_V = np.array([[-0.57536414,0.,0.],\n",
    "                    [np.NINF,0.,0.]])\n",
    "\n",
    "correct_P = np.array([[0, 0, 1],\n",
    "                      [0, 0, 1]])\n",
    "\n",
    "\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check that the output is a numpy array\n",
    "assert_equal(type(viterbi_rest(dummy_V,\n",
    "                               dummy_A, \n",
    "                               dummy_B, \n",
    "                               dummy_test, \n",
    "                               dummy_tags_vocab, \n",
    "                               dummy_words_vocab)[0]),np.ndarray)\n",
    "\n",
    "assert_equal(type(viterbi_rest(dummy_V,\n",
    "                               dummy_A, \n",
    "                               dummy_B, \n",
    "                               dummy_test, \n",
    "                               dummy_tags_vocab, \n",
    "                               dummy_words_vocab)[1]),np.ndarray)\n",
    "\n",
    "assert_equal(viterbi_rest(dummy_V,\n",
    "                          dummy_A, \n",
    "                          dummy_B,\n",
    "                          dummy_test,\n",
    "                          dummy_tags_vocab,\n",
    "                          dummy_words_vocab)[0].shape, (len(dummy_tags_vocab), len(dummy_test)))\n",
    "             \n",
    "assert_equal(viterbi_rest(dummy_V,\n",
    "                          dummy_A, \n",
    "                          dummy_B, \n",
    "                          dummy_test, \n",
    "                          dummy_tags_vocab, \n",
    "                          dummy_words_vocab)[1].shape, (len(dummy_tags_vocab), len(dummy_test)))\n",
    "\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check the function is giving out the right probabilities\n",
    "assert_almost_equal(viterbi_rest(dummy_V,\n",
    "                                 dummy_A,\n",
    "                                 dummy_B,\n",
    "                                 dummy_test,\n",
    "                                 dummy_tags_vocab,\n",
    "                                 dummy_words_vocab)[0][0][1], np.log(0.035), 2)\n",
    "\n",
    "# check the function is giving out the right backpointers\n",
    "assert_array_equal(viterbi_rest(dummy_V,\n",
    "                                 dummy_A,\n",
    "                                 dummy_B,\n",
    "                                 dummy_test,\n",
    "                                 dummy_tags_vocab,\n",
    "                                 dummy_words_vocab)[1], correct_P)\n",
    "\n",
    "# checking that the function is working correctly with unknown words\n",
    "dummy_test2 = ['red','right','leg']\n",
    "dummy_V2 = np.array([[-0.28768207,  -111., -111.],\n",
    "                     [-1.38629436,  -111., -111.]])\n",
    "\n",
    "correct_V2 = np.array([[-0.28768207, -3.06027079, -4.44656515],\n",
    "                       [-1.38629436,        np.NINF, -3.34795286]])\n",
    "\n",
    "correct_P2 = np.array([[0, 0, 1],\n",
    "                       [0, 0, 1]])\n",
    "\n",
    "assert_almost_equal(viterbi_rest(dummy_V2,\n",
    "                                  dummy_A,\n",
    "                                  dummy_B, \n",
    "                                  dummy_test2, \n",
    "                                  dummy_tags_vocab,\n",
    "                                  dummy_words_vocab)[0], correct_V2, 3)\n",
    "\n",
    "assert_array_equal(viterbi_rest(dummy_V2,\n",
    "                                  dummy_A,\n",
    "                                  dummy_B, \n",
    "                                  dummy_test2, \n",
    "                                  dummy_tags_vocab,\n",
    "                                  dummy_words_vocab)[1], correct_P2)\n",
    "\n",
    "\n",
    "# checking that the function is working as intended with equal probabilities\n",
    "dummy_A_eq = np.array([[1/2,1/2,0],\n",
    "                       [1/2,0,1/2],\n",
    "                       [0,1/2,1/2]])\n",
    "\n",
    "dummy_B_eq = np.array([[1/2,1/2],\n",
    "                       [1/2,1/2]])\n",
    "\n",
    "dummy_test_eq = [\"W1\", \"W2\"]\n",
    "dummy_tags_vocab_eq = [\"T1\", \"T2\"]\n",
    "dummy_words_vocab_eq = ['W1','W2']\n",
    "\n",
    "dummy_V_eq = np.array([[-1.38629436, -111.],\n",
    "                       [-1.38629436, -111.]])\n",
    "\n",
    "correct_P_eq = [[0, 0],\n",
    "                [1, 0]]\n",
    "\n",
    "correct_V_eq = np.array([[-1.38629436, -2.77258872],\n",
    "                         [-1.38629436, -2.77258872]])\n",
    "\n",
    "assert_almost_equal(viterbi_rest(dummy_V_eq,\n",
    "                                 dummy_A_eq,\n",
    "                                 dummy_B_eq,\n",
    "                                 dummy_test_eq, \n",
    "                                 dummy_tags_vocab_eq,\n",
    "                                 dummy_words_vocab_eq)[0], correct_V_eq, 3)\n",
    "\n",
    "assert_array_equal(viterbi_rest(dummy_V_eq,\n",
    "                                 dummy_A_eq,\n",
    "                                 dummy_B_eq,\n",
    "                                 dummy_test_eq, \n",
    "                                 dummy_tags_vocab_eq,\n",
    "                                 dummy_words_vocab_eq)[1], correct_P_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5433109f28e3f37dc2f390e4d4ea662f",
     "grade": false,
     "grade_id": "cell-857f2913d3f09ad9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.1.3 <a class=\"anchor\" id=\"subtask_4_2_3\"></a>\n",
    "#### Traceback the Tag sequence (1 point)\n",
    "Steps 10-11\n",
    "\n",
    "Take the backpointer matrix and trace the most probable tag sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1673cacd9fff8129ae88eb9873de8fe2",
     "grade": false,
     "grade_id": "cell-f40b5f19496064fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trace_backpointers(backpointer_table, tags_vocab):\n",
    "    \"\"\" Traces tag sequence from a backpointer table P\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    backpointer_table : a numpy array\n",
    "        a matrix P with backpointers\n",
    "    tags_vocab : a list of strings\n",
    "        a list of UD tag labels\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_path : a list of strings\n",
    "        most probable tags for the words in a test sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    best_path = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0432407e25e16f54149f9135712e53f",
     "grade": true,
     "grade_id": "cell-e35aa2f929717bcb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_A = np.array([[3/4,1/4,0],\n",
    "                    [1/4,3/4,0],\n",
    "                    [0,0,1.]])\n",
    "\n",
    "dummy_B = np.array([[0,1],\n",
    "                    [3/4,0],\n",
    "                    [1/4, 0]])\n",
    "\n",
    "dummy_test = ['red','right','hand']\n",
    "dummy_tags_vocab = [\"ADJ\", \"NOUN\"]\n",
    "dummy_words_vocab = ['hand','red','right']\n",
    "\n",
    "correct_dummy_tags = ['ADJ', 'ADJ', 'NOUN']\n",
    "correct_P = np.array([[0, 0, 1],\n",
    "                      [0, 0, 1]])\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check that we get a list of strings\n",
    "assert_equal(type(trace_backpointers(correct_P, dummy_tags_vocab)), list)\n",
    "assert_equal(type(trace_backpointers(correct_P, dummy_tags_vocab)[0]), str)\n",
    "# check the length\n",
    "assert_equal(len(trace_backpointers(correct_P, dummy_tags_vocab)), correct_P.shape[1])\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "assert_equal(trace_backpointers(correct_P, dummy_tags_vocab), correct_dummy_tags)\n",
    "\n",
    "P2 = np.array([[1, 0, 1],\n",
    "               [0, 2, 1],\n",
    "               [1, 1, 1]])\n",
    "\n",
    "assert_equal(trace_backpointers(P2, [\"T1\", \"T2\",\"T3\"]), [\"T2\",\"T3\",\"T2\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8408087c9020a13223cdef86410eb42c",
     "grade": false,
     "grade_id": "cell-b2f21c3f133a4264",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluate HMM\n",
    "Run the cell below to get an accuracy score for your HMM tagger and to plot the confusion matrix. Your accuracy should be better than the baseline! Note that the computations take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1ba583f7941bde894964284b970605f",
     "grade": false,
     "grade_id": "cell-329710d59cc3bbad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hmm_tags_predicted = []\n",
    "for i in range(len(test_words)):\n",
    "    V = viterbi_first_column(A, B, test_words[i], tags_vocab, words_vocab)\n",
    "    _, P = viterbi_rest(V, A, B, test_words[i], tags_vocab, words_vocab)\n",
    "    hmm_tags_predicted.append(trace_backpointers(P, tags_vocab))\n",
    "\n",
    "print(accuracy_score([tag for sentence in test_tags for tag in sentence],\n",
    "                     [tag for sentence in hmm_tags_predicted for tag in sentence]))\n",
    "\n",
    "cm_hmm = create_confusion_matrix(test_tags, hmm_tags_predicted)\n",
    "plot_confusion_matrix(cm_hmm, tags_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "accf10b197e5c409be2dcc7ca3becb48",
     "grade": false,
     "grade_id": "cell-4b4885aa04a6f6dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4.2  <a class=\"anchor\" id=\"subtask_4_2\"></a>\n",
    "### Compare HMM and Baseline (1 point)\n",
    "\n",
    "In the cell below, briefly decribe the differences in the performance of our HMM and the baseline model. What ar ethe tags that are handled better by the HMM? What are the cases where basline was better? Why? (0.5 points). What can be done to further improve the HMM model (0.5 points)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54e1df81e41ea42f9c9e9c50885de841",
     "grade": true,
     "grade_id": "cell-340e6551f6785f11",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea885b9ac2c1aece848a391f787d3f21",
     "grade": false,
     "grade_id": "cell-aa8e8a6df8cf7542",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checklist before submission <a class=\"anchor\" id=\"checklist\"></a>\n",
    "### 1\n",
    "To make sure that you didn't forget to import some package or to name some variable, press **Kernel -> Restart** and then **Cell -> Run All**. This way your code will be run exactly in the same order as during the autograding.\n",
    "### 2\n",
    "Click the **Validate** button in the upper menu to check that you haven't missed anything. Be careful now (Viterbi takes time, you might need to validate through the terminal).\n",
    "### 3\n",
    "To submit the notebook, click on the **jupyterhub** logo in the upper left part of the window, choose the **Assignments** folder, and press **submit**. You can submit multiple times, only the last one counts.\n",
    "### 4\n",
    "Please fill in the feedback form in the [Assignment](https://mycourses.aalto.fi/mod/questionnaire/view.php?id=1122379) section of Mycoures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
