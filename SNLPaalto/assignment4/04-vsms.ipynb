{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d69fb33d41ce611853b12a99ffe2dd18",
     "grade": false,
     "grade_id": "cell-8570cfd12f5ca3ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 4: Vector Space Models\n",
    "\n",
    "## Released: 06.02.2024\n",
    "## Deadline: 16.02.2024 at 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddc4ce1fb60b9554ade0837ff24a2841",
     "grade": false,
     "grade_id": "cell-d8ae628155888deb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this assignment, we're going to represent documents as points in some high-dimensional vector space. We will try to model their semantics so that similar documents would be close in that space. We're going to work with the song lyrics of three artists. More precisely, our goal is to build a system that can suggest what artist could have written a song when presented with the lyrics of a new, previously unseen, song (with a query). We're going to use several approaches to representing songs (several Vector Space Models):\n",
    "1. songs as sparse vectors of lemma counts\n",
    "2. songs as sparse vectors weighted by tf-idf\n",
    "3. songs as sparse vectors weighted by PPMI\n",
    "4. songs as dense vectors derived from sparse vectors of lemma counts\n",
    "5. songs as dense vectors derived from sparse vectors weighted by tf-idf\n",
    "6. songs as dense vectors derived from sparse vectors weighted by PPMI\n",
    "\n",
    "Hopefully, your models will be able to tell the artists apart! We will evaluate the models by looking at what songs are the closest to a new song in these vector spaces. The perfect system should find only the songs by the same artist. Don't worry if some of the terms above sound confusing, they are all going to be explained below. \n",
    "\n",
    "Every song will be represented as a vector of values related to word frequencies in this song. The simple intuition behind the method is that word frequencies in a song capture what a corresponding document is about, its meaning. So documents with the similar words should be similar (similar documents will have high values/counts in the same dimensions/word indices). This approach is count-based. Vector spaces are typically constructed from some kind of co-occurrence matrix. We will be using a term-document matrix in this assignment.\n",
    "\n",
    "In general, the creation of count-based vector models consists of the following steps:\n",
    "1. Linguistic processing of text.\n",
    "2. Frequency matrix building.\n",
    "3. Mathematical processing of the matrix elements.\n",
    "4. Dimensionality reduction\n",
    "5. Evaluation\n",
    "\n",
    "We're going to use song lyrics of three artists working in different music genres: **Pulp** (britpop, indie pop), **Princess Nokia** (rap), **At the Drive-In** (post-hardcore, emo).\n",
    "\n",
    "Albums for training:\n",
    "\n",
    "* Pulp: Different Class, His'n'Hers, We Love Life\n",
    "* Princess Nokia: Metallic butterfly, Everything sucks, Everything is beautiful\n",
    "* At the Drive-In: Acrobatic Tenement, Relationship of Command, Vaya\n",
    "\n",
    "\n",
    "Albums for testing:\n",
    "\n",
    "* Pulp: This Is Hardcore\n",
    "* Princess Nokia: 1992\n",
    "* At the Drive-In: In/Casino/Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a58f3ead97781b859b2e53d45446929",
     "grade": false,
     "grade_id": "cell-f3ea63a2d24a1037",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Task 1: Text pre-processing](#task_1)\n",
    "    * [Step 1.1: Tokenization and normalization](#subtask_1_1)\n",
    "* [Task 2: Term-document matrix creation](#task_2)\n",
    "    * [Step 2.1: Create a term-document matrix](#subtask_2_1)\n",
    "* [Task 3: Mathematical processing of matrix elements](#task_3)\n",
    "    * [Step 3.1: tf-idf](#subtask_3_1)\n",
    "    * [Step 3.2: PPMI](#subtask_3_2)\n",
    "* [Task 4: Place queries into the right vector space](#task_4)\n",
    "    * [Step 4.1: Term-document matrix for queries](#subtask_4_1)\n",
    "* [Task 5: Quality evaluation](#task_5)    \n",
    "    * [Step 5.1: Find N closest documents to a quesry song](#subtask_5_1)\n",
    "    * [Step 5.2: Compare models](#subtask_5_2)\n",
    "        * [Step 5.2.1: Count Negatives and Positives](#subsubtask_5_2_1)\n",
    "        * [Step 5.2.2: Compute the metrics](#subsubtask_5_2_2)\n",
    "    * [Step 5.3: Choose the best model](#subtask_7_2)\n",
    "* [Checklist before submission](#checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afac8f0623382c12748ce0652b2a2a85",
     "grade": false,
     "grade_id": "cell-5044486f6a1cd371",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 1 <a class=\"anchor\" id=\"task_1\"></a>\n",
    "## Text pre-processing\n",
    "## 1.1 <a class=\"anchor\" id=\"subtask_1_1\"></a>\n",
    "### Tokenization and normalization (1 point)\n",
    "\n",
    "As you already know, we always start by preparing text data to fit our needs. The first step of our process will be **tokenization** (as usual). The second step will be **normalization**. Normalization helps reduce superficially different strings to one spelling. For example, strings that differ in their capitalization: \"Run\" and \"run\", or the strings that differ in their grammatical form: \"cat\" and \"cats\". these different strings of characters often convey essentially identical meanings, so it makes sense to unify them into one string in order to get the full distributional statistics. The most common types of normalization for document representations are **lowercasing**, **stemming** and **lemmatization**.\n",
    "\n",
    "**Stemmers** remove morphological affixes from words, leaving only the word stem. For example, words \"jumping\", “jumped”, and “jumper” will be reduced to the stem \"jump\". Stemming is just an approximation of a proper morphological analysis and uses a set of rules of thumb, so for forms like \"fly\", \"flies\", \"flying\" it will assign \"fli\" and it's not a proper English word. Stemming works well for languages like English, but might not be optimal for morphologically-rich languages like Finnish. **Lemmatizers** assign a group of word forms their lemma (dictionary form), so in the case of \"fly\"/\"flies\"/\"flying\" we get \"fly\", they are doing proper morphological analysis.\n",
    "\n",
    "We will also remove **stop words** from the songs. These are functional words like \"the\" or \"in\" that are present in all texts, so they don't help in distinguishing similar texts from different ones.\n",
    "\n",
    "In this assignment, we're going to use [Stanza](https://stanfordnlp.github.io/stanza/index.html) for tokenization and lemmatization.\n",
    "\n",
    "Create a function in a cell below. It should take the name of the text document:\n",
    "1. read it\n",
    "2. tokenize and lemmatize it\n",
    "3. lowercase lemmas\n",
    "4. remove lemmas that are present in a stop word list\n",
    "\n",
    "\n",
    "HINT1: look [here](https://github.com/stanfordnlp/stanza/blob/master/demo/Stanza_Beginners_Guide.ipynb) for Stanza tutorial\n",
    "\n",
    "HINT2: lowercase only after lemmatization, case affects lemmatization result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27f55aab3eda8cd42997746a615c37ec",
     "grade": false,
     "grade_id": "cell-028c78a8e5a12c2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install stanza\n",
    "import stanza\n",
    "print(\"Downloading English model...\")\n",
    "stanza.download('en', verbose=False)\n",
    "\n",
    "\n",
    "def tokenize_and_normalize(file_name, stopwords):\n",
    "    \"\"\"Tokenizes, lemmatizes, lowercases and removes stop words.\n",
    "    \n",
    "    this function takes in a path to a song, reads the song file,\n",
    "    tokenizes it into words, then lemmatizes and lowercases these words.\n",
    "    finally, stopwords given to the function are removed from the list of song lemmas\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to a text file\n",
    "    stopwords : list of strings\n",
    "        stopwords that should be removed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normalized_song : list of strings\n",
    "        a song represented as a list of its lemmas\n",
    "    \"\"\"\n",
    "    \n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize, lemma',  verbose=False)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return normalized_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29ebe23462b56a0ce5c4da962bae709e",
     "grade": true,
     "grade_id": "cell-ccd00641165e99e2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_song_path = \"/coursedata/03-vsms/dummy_song.txt\"\n",
    "dummy_stop_words = ['lalala', '.']\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(tokenize_and_normalize(dummy_song_path,dummy_stop_words)), list)\n",
    "# check that it's a list of strings\n",
    "assert_equal(type(tokenize_and_normalize(dummy_song_path,dummy_stop_words)[0]), str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "correct_normalized_dummy_song = ['this','be','my','awesome','song',\n",
    "                                 'i','sing','everywhere','i','go',',',\n",
    "                                 'lalada',',','dalala',',','yep','yep','!','ohhhhh','ohhhhhh','yeaaahhhhh',\n",
    "                                 'and','i','have','parted','with','my','sanity','!']\n",
    "assert_equal(tokenize_and_normalize(dummy_song_path,dummy_stop_words), correct_normalized_dummy_song)\n",
    "\n",
    "# SANITY CHECK FOR THE SONG DATA\n",
    "song1 = '/coursedata/03-vsms/songs_train/at_the_drive_in_vaya/6_300_mhz.txt'\n",
    "assert_equal(len(tokenize_and_normalize(song1, dummy_stop_words)), 174)\n",
    "assert(all([l.lower() for l in tokenize_and_normalize(song1, dummy_stop_words)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4626e0e74869096a888aee567b402ac",
     "grade": false,
     "grade_id": "cell-897cbf1afc516283",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Now we can prepare our songs. \n",
    "Run the cell below to get a list of pre-processed songs. We're also going to keep track for each artist of what are the indices of songs in this list.\n",
    "\n",
    "The songs will be stored in `normalized_songs_train`. The dictionary with indices from this list will be kept in `normalized_songs_index_train` (the keys are artists and he values are lists of indices in the \n",
    "The songs will be stored in `normalized_songs_train`). We will use `normalized_songs_index_train` to keep track which song belongs to which artist.\n",
    "\n",
    "Note that the processing is going to take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d710b86b0a381540cfd094a8856a03c",
     "grade": false,
     "grade_id": "cell-2fcc51ebdae2230f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "import ssl\n",
    "print(\"Downloading stop words...\")\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def add_song(song_dict, artist, index):\n",
    "    \"\"\"Writes a song index into a dictiionary of artists and their song indices\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    song_dict: dict {artist:list of indices}\n",
    "        dictionary of artist names with a list of indices of their songs\n",
    "    artist : str\n",
    "        artist name - a key to which to add value\n",
    "    index : int\n",
    "        value, an index of a song in a list of all songs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    song_dict : dict {atrist:lits of song indices}\n",
    "        song_dict with one new index added\n",
    "    \"\"\"\n",
    "    \n",
    "    if song_dict[artist] == None:\n",
    "        song_dict[artist] = [index]\n",
    "    else:\n",
    "        song_dict[artist].append(index)   \n",
    "    return song_dict\n",
    "\n",
    "\n",
    "# collecting the paths to albums and their songs\n",
    "train_songs = glob.glob('/coursedata/03-vsms/songs_train/*/*')\n",
    "# getting stopwords\n",
    "stop_words_english = stopwords.words('english')\n",
    "\n",
    "# lets remember indices for each song, we'll need them later\n",
    "normalized_songs_index_train  = {'pulp':None, 'princess_nokia':None, 'at_the_drive_in':None}\n",
    "normalized_songs_train = []\n",
    "\n",
    "\n",
    "for i, song_path in enumerate(train_songs):\n",
    "    \n",
    "    normalized_songs_train.append(tokenize_and_normalize(song_path, stop_words_english))\n",
    "    \n",
    "    if 'pulp' in song_path:\n",
    "        normalized_songs_index_train = add_song(normalized_songs_index_train,'pulp', i)\n",
    "    \n",
    "    if 'princess_nokia' in song_path:\n",
    "        normalized_songs_index_train = add_song(normalized_songs_index_train,'princess_nokia',i)\n",
    "     \n",
    "    if 'at_the_drive_in' in song_path:\n",
    "        normalized_songs_index_train = add_song(normalized_songs_index_train,'at_the_drive_in',i)\n",
    "            \n",
    "print('There are:', len(normalized_songs_index_train['pulp']), 'songs by Pulp')\n",
    "print('There are:', len(normalized_songs_index_train['princess_nokia']), 'songs by Princess Nokia')\n",
    "print('There are:', len(normalized_songs_index_train['at_the_drive_in']), 'songs by At the Drive-In')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f392961a422a3a44b527f7dcedc521f6",
     "grade": false,
     "grade_id": "cell-f7c984365f51779c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2 <a class=\"anchor\" id=\"task_2\"></a>\n",
    "## Term-document matrix creation\n",
    "## 2.1 <a class=\"anchor\" id=\"subtask_2_1\"></a>\n",
    "### Create a term-document matrix (3 points)\n",
    "Now that step one (linguistic processing) is complete, we can go on to compute statistics from the documents.\n",
    "\n",
    "We will be using **term-document matrix**. Formally, in a term-document matrix $X$, each column $x_i$ corresponds to a document $D_i$, each row $x_j$ corresponds to a term $T_j$, and an element $x_{ij}$ is the frequency of a term $T_j$ in a document $D_i$. The terms rows (lemmas) should be organized in the alphabetic order. The song columns should be the same as in the list given to the function.\n",
    "\n",
    "Write a function that takes in a list of pre-processed songs and puts their statistics into a term-document matrix. Keep also a list of lemma types corresponding to the rows, we'll need them later.\n",
    "\n",
    "\n",
    "HINT1: you can use collections.Counter to collect statistics for each song.\n",
    "\n",
    "HINT2: you can look at the tests, to examine some useful examples of functions performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb74983f64c139783af1f866a5eb773d",
     "grade": false,
     "grade_id": "cell-50bc668ab6453a56",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_term_doc_matrix(songs_normalized):\n",
    "    \"\"\" Constructs a frequency term-document matrix\n",
    "    \n",
    "    this function takes in a list of songs and returns a term-document matrix\n",
    "    the rows are lemma types, the columns are songs \n",
    "    the rows should be sorted alphabetically\n",
    "    the order of the columns should be preserved as it's given in songs_normalized\n",
    "    the cell values are a number of times a lemma was seen in a song\n",
    "    the value should be zero, if a lemma is absent from a song\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    songs_normalized : a list of lists of strings [['a','a','b'], ['a','b','c']]\n",
    "        a list of songs represented as a list of lemmas\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matrix : numpy array\n",
    "        a matrix where columns are songs and rows are lemma types,\n",
    "        the cells of the matrix contain lemma counts in a song,\n",
    "        the lemmas for rows are sorted alphabetically\n",
    "        for the example above it will be:\n",
    "            np.array([[2,1],\n",
    "                      [1,1],\n",
    "                      [0,1]])\n",
    "        \n",
    "    sorted_vocab : list of strings\n",
    "        a list of all the lemma types used in all songs (the rows of our matrix)\n",
    "        the words should be strings sorted alphabetically\n",
    "        for the example above it should be ['a','b','c']\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return matrix, sorted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aeb8f15c3326e782c8ea48db79493082",
     "grade": true,
     "grade_id": "cell-8c9de22bc0b55f2a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_songs = [['la','la','la','oh',',','woo',\"uuuuuh\"],\n",
    "               ['oh', 'la','la','la',\"tarara\",'tadada', 'blaaa', 'blaaa', '!', '!', '!']]\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(create_term_doc_matrix(dummy_songs)[0].shape, (9, 2))\n",
    "# check that the matrix is a numpy ndarray\n",
    "assert_equal(type(create_term_doc_matrix(dummy_songs)[0]), np.ndarray)\n",
    "# check that the vocabulary is a list\n",
    "assert_equal(type(create_term_doc_matrix(dummy_songs)[1]), list)\n",
    "# check that the vocabulary is a list of strings\n",
    "assert_equal(type(create_term_doc_matrix(dummy_songs)[1][0]), str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the vocabulary is sorted properly\n",
    "assert_equal(create_term_doc_matrix(dummy_songs)[1], ['!', ',', 'blaaa', 'la', 'oh', 'tadada', 'tarara', 'uuuuuh', 'woo'])\n",
    "# check the count of 'la' in the first song\n",
    "assert_equal(create_term_doc_matrix(dummy_songs)[0][3][0], 3)\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_td_dummy_matrix = np.array([[0., 3.],\n",
    "                                    [1., 0.],\n",
    "                                    [0., 2.],\n",
    "                                    [3., 3.],\n",
    "                                    [1., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [1., 0.]])\n",
    "\n",
    "assert_array_equal(create_term_doc_matrix(dummy_songs)[0], correct_td_dummy_matrix)\n",
    "\n",
    "# SANITY CHECK FOR THE SONG DATA\n",
    "# check the shape of the matrix\n",
    "assert_equal(create_term_doc_matrix(normalized_songs_train)[0].shape, (3264, 99))\n",
    "# check the first and last lemma\n",
    "assert_equal(create_term_doc_matrix(normalized_songs_train)[1][0], '!')\n",
    "assert_equal(create_term_doc_matrix(normalized_songs_train)[1][-1], '—')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c00cc3c0d35480967a0fecd2a2ded08",
     "grade": false,
     "grade_id": "cell-36b8993f83640303",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's make a term-frequency matrix for our songs\n",
    "\n",
    "Run the cell below to collect the statistics for the songs in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a14c346407ef402110e159db9458964e",
     "grade": false,
     "grade_id": "cell-df635a8414aeb58a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "td_matrix, sorted_vocabulary = create_term_doc_matrix(normalized_songs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "653588172de8362473e154d9c2ef4ce9",
     "grade": false,
     "grade_id": "cell-2c9e25bb552215f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 3 <a class=\"anchor\" id=\"task_3\"></a>\n",
    "## Mathematical processing of matrix elements\n",
    "\n",
    "Simple raw frequency counts are not the best at the task of measuring the similarity between documents. The information theory states that the more probable the event, the less information it contains. We've already removed the stop words, but we can go even further. For the VSM models, shared frequent words like \"run\" or \"boy\" in two documents are less discriminative for semantics than shared surprising words like \"kerning\" or \"typeface\". Thus, it is only logical to give more weight to rare words and take some weight from frequent words. \n",
    "\n",
    "To downplay the value of common words some weighting schemes can be applied to a matrix. Two popular approaches are **tf-idf** and **PPMI**. They both are based on the information theory idea that surprising events should carry more weight than expected events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8512235c5106fe1a98cb82a002ffa0a9",
     "grade": false,
     "grade_id": "cell-473e71d95233c90e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.1 <a class=\"anchor\" id=\"subtask_3_1\"></a>\n",
    "### tf-idf (3 points)\n",
    "\n",
    "The **term frequency–inverse document frequency (tf-idf)** algorithm is a weighting scheme for term-document matrices. It captures the intuition that when a word is frequent in a document but is rare in other documents, it carries a lot of information specific for that particular document, thus it should get more weight. In practice, the **tf-idf** is a combination of two statistics: **term frequency** and **inverse document frequency**. \n",
    "\n",
    "\n",
    "**Term frequency** - the number of times a word type $t$ appears in a document $d$\n",
    "\n",
    "$tf_{t,d} = count(t, d)$\n",
    "\n",
    "\n",
    "The second statistic of tf-idf is the **inverse document frequency**. It is based on a notion that a less specific term would be used in more documents in a collection than the more specific ones, so the weight of a less specific term should be downplayed. Formally, **idf** is the total number $N$ of documents in a collection, divided by the number of documents $d_{ft}$ containing the term $t$. The fewer documents contain the term, the larger the **idf** becomes. Due to a large number of documents in many collections, $idf$ is usually squashed with a log function. So the resulting term can be obtained in the following way:\n",
    "\n",
    "**Document frequency** $df_t$ - the number of documents containing $t$\n",
    "\n",
    "**Inverse document frequency** - the total number $N$ of documents divided by the $df_t$\n",
    "\n",
    "$idf_t = \\log_{10}(\\frac{N}{d_{ft}})$\n",
    "\n",
    "Finally, the *tf-idf* score for a word is just a term frequency multiplied by the inverse document frequency:\n",
    "\n",
    "$w_{t,d} = tf_{t,d} × id_{ft}$\n",
    "\n",
    "#### How to weigh a query document with tf-idf?\n",
    "When you have a new document that you want to compare to the documents in the collection, you need to represent it the same way as your old documents. Suppose, your query document already looks like a vector with the dimensions as the vocabulary of your collection. That is, we disregard any new words that the query document might contain.\n",
    "\n",
    "The **tf** of this query document **depends only on the query** itself and is computed the same way as for the training documents. \n",
    "\n",
    "The **idf** is different. It **depends only on the statistics of the documents in our collection**. That means, we don't add the new document to the statistics, but just use the number we had for the existing collection.\n",
    "\n",
    "#### task description\n",
    "Write a function, that takes in a term-document matrix and weights it with the **tf-idf** scheme. To save us time later, our function will both give out a matrix of tf-idf weighted values and a vector of idf values for words in the collection. We will use this vector when representing new documents in the same vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94ddcaf3b7d0e5d069674424b4a14ba8",
     "grade": false,
     "grade_id": "cell-33fcd5d339672048",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_idf(td_matrix):\n",
    "    \"\"\" Weighs a term-document matrix of raw counts with tf-idf scheme\n",
    "    \n",
    "    this function takes in a term-document matrix as a numpy array, \n",
    "    and weights the scores with the tf-idf algorithm described above.\n",
    "    idf values are modified with log_10\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    td_matrix : numpy array \n",
    "        a matrix where columns are songs and \n",
    "        rows are word counts in a song\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tf_idf_matrix : numpy array \n",
    "        a matrix where columns are songs and \n",
    "        rows are word tf-idf values in a song\n",
    "        \n",
    "    idf_vector : numpy array of shape (vocabulary-size, 1)\n",
    "        a vector of idf values for words in the collection. the shape is (vocabulary-size, 1)\n",
    "        this vector will be used to weight new query documents\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return tf_idf_matrix, idf_vector      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1a1050c02bf5bd88fed14416ac90afc",
     "grade": true,
     "grade_id": "cell-94805d8b0ce03198",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal, assert_allclose, assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_songs = [['la','la','la','la','oh',',','woo',\"uuuuuh\"],\n",
    "               ['oh', 'la','la','la',\"tarara\",'tadada', 'bla', 'bla', 'bla','bla','bla', '!']]\n",
    "\n",
    "\n",
    "sorted_dummy_vocab = ['!', ',', 'bla', 'la', 'oh', 'tadada', 'tarara', 'uuuuuh', 'woo']\n",
    "correct_td_dummy_matrix = np.array([[0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [0., 5.],\n",
    "                                    [4., 3.],\n",
    "                                    [1., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [1., 0.]])\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(tf_idf(correct_td_dummy_matrix)[0].shape, (9, 2))\n",
    "# check the shape of the idf vector\n",
    "assert_equal(tf_idf(correct_td_dummy_matrix)[1].shape, (9, 1))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "# pay attention to the \"la\" row: its values are zeros now. make sure you understand why.  \n",
    "\n",
    "correct_tf_idf_dummy_matrix = np.array([[0.,0.30103],\n",
    "                                        [0.30103,0.],\n",
    "                                        [0.,1.50514998],\n",
    "                                        [0.,0.],\n",
    "                                        [0.,0.],\n",
    "                                        [0.,0.30103],\n",
    "                                        [0.,0.30103],\n",
    "                                        [0.30103,0.],\n",
    "                                        [0.30103,0.]])\n",
    "\n",
    "correct_idf_dummy_vector = np.array([[0.30103],\n",
    "                                     [0.30103],\n",
    "                                     [0.30103],\n",
    "                                     [0.     ],\n",
    "                                     [0.     ],\n",
    "                                     [0.30103],\n",
    "                                     [0.30103],\n",
    "                                     [0.30103],\n",
    "                                     [0.30103]])\n",
    "\n",
    "assert_allclose(tf_idf(correct_td_dummy_matrix)[0], correct_tf_idf_dummy_matrix, rtol=1e-3)\n",
    "assert_allclose(tf_idf(correct_td_dummy_matrix)[1], correct_idf_dummy_vector, rtol=1e-3)\n",
    "\n",
    "\n",
    "# SANITY CHECK FOR THE SONG DATA\n",
    "# check the first value of the idf vector\n",
    "assert_almost_equal(tf_idf(td_matrix)[1][0][0], 0.8816918422907132, 3)\n",
    "# check the last value of the idf vector\n",
    "assert_almost_equal(tf_idf(td_matrix)[1][-1][0], 1.6946051989335686, 3)\n",
    "# check the [0][0] element of the tf-idf matrix\n",
    "assert_equal(tf_idf(td_matrix)[0][0][3], 0)\n",
    "# check the [0][3] element of the tf-idf matrix\n",
    "assert_almost_equal(tf_idf(td_matrix)[0][0][2], 0.8816918422907132, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5213a23be6da4739e4c6e10e91ed5bdf",
     "grade": false,
     "grade_id": "cell-b9180e4bcb31083e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.2 <a class=\"anchor\" id=\"subtask_3_2\"></a>\n",
    "### PPMI (3 points)\n",
    "\n",
    "An alternative weighting scheme to $tf-idf$ is **Positive Pointwise Mutual Information** (PPMI). When using PPMI scheme, we are asking how more frequently a word occurs in a document than it would have been expected to appear there by chance. The more unexpected this occurrence is, the more weight is given. For example, we might expect words like *run* and *boy* to appear in many documents, but words like *kerning* or *typeface* to appear only in texts about typography.\n",
    "\n",
    "**PPMI** is a variation of **Pointwise Mutual Information (PMI)**, in which all negative PMI values are replaced with zeros. \n",
    "\n",
    "The mutual information compares the probability of observing two events $x$ and $y$ together to the probability of observing $x$ and $y$ independently. Based on this definition of mutual information, PMI between a term $t$ and a document $d$ is defined in the following way:\n",
    "\n",
    "$PMI_{t,d} = \\log_{2}(\\frac{P(t,d)}{P(t)P(d)})$\n",
    "\n",
    "In the case of term-document matrix, the numerator $P(t,d)$ denotes how often a term $t$ was observed in a document $d$, the denominator $P(t)P(d)$ expresses how often we would see a term $t$ in a document $d$ if their probabilities were independent of each other. The larger the numerator, the more weight the pair gets because it means the word occurs in this document more often than by a pure chance.\n",
    "\n",
    "Let's look at it a bit closer:\n",
    "- $N$ - total counts of all the words in the collection of documents\n",
    "- $C_{t,d}$ - count of a term t in a document $d$\n",
    "- $C_t$ - total count of a term t in the whole document collection (the more we've seen it, the more likely it is to be seen again)\n",
    "- $C_d$ - total count of all words in a document $d$ (the length of a document - the more words a document has, the more likely for it to include the word we're interested in)\n",
    "- $PMI_{t,d} = \\log_{2}(\\frac{\\frac{C_{t,d}}{N}}{\\frac{C_t}{N}\\frac{C_d}{N}})$\n",
    "\n",
    "$PPMI_{t,d} = \\max(PMI_{t,d},0)$\n",
    "\n",
    "### How to weigh a query document with PPMI?\n",
    "We are again in the same situation as with tf-idf: the new document vector should have the same number of elements (dimensions) as a vector in our collection, its dimensions should correspond to the dimension of our collection vectors, and the values should be comparable. To be able to weigh the frequencies in the new documents, we are going to need the $C_t$ statistic and $N$ to come **from the collection**. The $C_{t,d}$ and $C_d$ values would come **from the query vector**.\n",
    "\n",
    "#### task description\n",
    "\n",
    "Write a function that can weight with **PPMI** scheme both train and test term-document matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8a84a3792c9426497520edee95d0dd8",
     "grade": false,
     "grade_id": "cell-1758b0ed2838db07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ppmi(td_matrix, query_matrix = None):\n",
    "    \"\"\"Weighs a term-document matrix of raw counts with the PPMI scheme\n",
    "    \n",
    "    this function takes in a term-document matrix as a numpy array, \n",
    "    and weights the scores with the PPMI scheme described above\n",
    "    use PMI values are modified with log_2\n",
    "    if term-document matrix for songs of a test set is given as well,\n",
    "    the functions returns only PPMI weighted test set (query) matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    td_matrix : numpy array \n",
    "        a matrix where columns are collection songs and \n",
    "        rows are word counts in a song\n",
    "    query_matrix : numpy array\n",
    "        a numpy array where columns are query songs and \n",
    "        rows are word counts in a song\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ppmi_matrix - a numpy array \n",
    "        a matrix where columns are songs and \n",
    "        rows are ppmi word values in a song\n",
    "    \"\"\"\n",
    "    if query_matrix is None:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return ppmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d436a85307238594ef239471e276b8a2",
     "grade": true,
     "grade_id": "cell-21dee6aeac2b6a25",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal, assert_allclose\n",
    "from nose.tools import assert_equal\n",
    "np.seterr(divide = 'ignore')\n",
    "\n",
    "\n",
    "dummy_songs = [['la','la','la','la','oh',',','woo',\"uuuuuh\"],\n",
    "               ['oh', 'la','la','la',\"tarara\",'tadada', 'bla', 'bla', 'bla','bla','bla', '!']]\n",
    "\n",
    "\n",
    "sorted_dummy_vocab = ['!', ',', 'bla', 'la', 'oh', 'tadada', 'tarara', 'uuuuuh', 'woo']\n",
    "correct_td_dummy_matrix = np.array([[0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [0., 5.],\n",
    "                                    [4., 3.],\n",
    "                                    [1., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [1., 0.],\n",
    "                                    [1., 0.]])\n",
    "\n",
    "td_dummy_query1 = np.array([[0.],\n",
    "                           [1.],\n",
    "                           [8.],\n",
    "                           [1.],\n",
    "                           [0.],\n",
    "                           [0.],\n",
    "                           [1.],\n",
    "                           [2.],\n",
    "                           [4.]])\n",
    "\n",
    "\n",
    "\n",
    "td_dummy_query2 = np.array([[0., 2., 2.],\n",
    "                           [1., 3., 2.],\n",
    "                           [8., 4., 2.],\n",
    "                           [1., 2., 2.],\n",
    "                           [0., 5., 2.],\n",
    "                           [0., 0., 2.],\n",
    "                           [1., 0., 2.],\n",
    "                           [2., 0., 2.],\n",
    "                           [4., 0., 2.]])\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(ppmi(correct_td_dummy_matrix).shape, (9, 2))\n",
    "assert_equal(ppmi(correct_td_dummy_matrix, td_dummy_query1).shape, (9, 1))\n",
    "assert_equal(ppmi(correct_td_dummy_matrix, td_dummy_query2).shape, (9, 3))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "# pay attention to the row of \"woo\" and the row of \"tarara\" why are the ppmi values different for diiferent documents? \n",
    "\n",
    "correct_ppmi_dummy_matrix = np.array([[0.        , 0.73696559],\n",
    "                                      [1.32192809, 0.        ],\n",
    "                                      [0.        , 0.73696559],\n",
    "                                      [0.51457317, 0.        ],\n",
    "                                      [0.32192809, 0.        ],\n",
    "                                      [0.        , 0.73696559],\n",
    "                                      [0.        , 0.73696559],\n",
    "                                      [1.32192809, 0.        ],\n",
    "                                      [1.32192809, 0.        ]])\n",
    "\n",
    "assert_allclose(ppmi(correct_td_dummy_matrix), correct_ppmi_dummy_matrix, rtol=1e-3)\n",
    "\n",
    "correct_ppmi_dummy_query1 = np.array([[0.        ],\n",
    "                                     [0.23446525],\n",
    "                                     [0.91253716],\n",
    "                                     [0.        ],\n",
    "                                     [0.        ],\n",
    "                                     [0.        ],\n",
    "                                     [0.23446525],\n",
    "                                     [1.23446525],\n",
    "                                     [2.23446525]])\n",
    "\n",
    "correct_ppmi_dummy_query2 = np.array([[0.        , 1.32192809, 1.15200309],\n",
    "                                   [0.23446525, 1.9068906 , 1.15200309],\n",
    "                                   [0.91253716, 0.        , 0.        ],\n",
    "                                   [0.        , 0.        , 0.        ],\n",
    "                                   [0.        , 1.64385619, 0.15200309],\n",
    "                                   [0.        , 0.        , 1.15200309],\n",
    "                                   [0.23446525, 0.        , 1.15200309],\n",
    "                                   [1.23446525, 0.        , 1.15200309],\n",
    "                                   [2.23446525, 0.        , 1.15200309]])\n",
    "\n",
    "assert_allclose(ppmi(correct_td_dummy_matrix, td_dummy_query1), correct_ppmi_dummy_query1, rtol=1e-3)\n",
    "assert_allclose(ppmi(correct_td_dummy_matrix, td_dummy_query2), correct_ppmi_dummy_query2, rtol=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a7f295d1aaf5ad1a58c1f0c5d21d7f1",
     "grade": false,
     "grade_id": "cell-ed45ea1e6f004fce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's weigh our term-dictionary matrix\n",
    "Run the cell below to collect the weighted statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95a127128656011969846600e19ed69d",
     "grade": false,
     "grade_id": "cell-0613026c7e3835cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "tf_idf_matrix, idf_vector = tf_idf(td_matrix)\n",
    "\n",
    "# PPMI\n",
    "ppmi_matrix = ppmi(td_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da69d1419dfc7e46eaa81e136e6edfcd",
     "grade": false,
     "grade_id": "cell-1bd0768d79b94e86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dense vectors\n",
    "\n",
    "Raw counts, tf-idf and PPMI are called **sparse representations**. That means they contain lots of zeros because most of the words never appear in every document.\n",
    "\n",
    "Sparse frequency matrices work well, but keeping all vector components can be very computationally expensive in some tasks, moreover, such frequency matrices can take up a lot of storage space. But more importantly, sparse vectors dedicate different dimensions for very similar words like *car* and *automobile* with no modeled relationship between the dimensions. It would be useful to model similar words in the same dimension. Such low-dimensional **dense representations**, vectors can be obtained by performing dimensionality reduction on sparse vectors.\n",
    "\n",
    "### LSI\n",
    "\n",
    "One of the popular methods to transform a sparse VSM into a dense VSM is based on **Truncated Singular Value Decomposition (SVD)**. **SVD** is a method for ranking the dimensions of a dataset by their \"importance\". The dimensions along which the data varies the most are considered more important than the dimensions with low variation. Meaning: we want dimensions that can help distingish between different vectors. **Truncated** means keeping only $k$ most important dimensions out of all.\n",
    "\n",
    "We can approximate our term-document matrix with one of lower rank using the **Truncated SVD**. This approximation matrix yields a new representation for each document in the collection. You can think of it as assigning documents to latent topics starting with topics that help to differenciate between the documents the best. So now the word rows are replaced with latent topic rows. We will need to transform our query documents into this low-rank representation as well, enabling us to compute query-document similarity scores in this low-rank representation. This process is known as **Latent Semantic Indexing (LSI)**. \n",
    "\n",
    "NOTE: We can view our term-document matrix not only as representation of documents, but also as the representation of words (similar words appear in similar documents). **Truncated SVD** can also be applied to assign words hiddent latent topics. This process is called **Latent Semantic Analysis (LSA)**.\n",
    "\n",
    "You can read about the details of **LSI** [here](https://www.engr.uvic.ca/~seng474/svd.pdf) or [here](https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf)\n",
    "\n",
    "\n",
    "### UMAP\n",
    "\n",
    "**Uniform Manifold Approximation and Projection** or **UMAP** for short is also a dimensionality reduction technique. It is mostly used for visulizing data. **UMAP** tries to map high dimensional data into lower dimension while preserving the distances between the objects. That means documents that were close in the original high dimensional space would still be close in the lower dimesional space. You can learn the intuition by watching [this video](https://www.youtube.com/watch?v=eN0wFzBA4Sc&ab_channel=StatQuestwithJoshStarmer).\n",
    "\n",
    "The two main parameters that control the mapping is n_components and n_neighbors. n_components tells the algorithm how many dimensions we want our final vectors to have. You can think about n_neighbors as the number of documents a document would have a similar distance to in lower dimensional approximation. If we put this parameter to 3, we ask the algorithm to focus on preserving the distances to the 2 nearest neighbours of each document. You can read more about the parameters [here](https://umap-learn.readthedocs.io/en/latest/parameters.html).\n",
    "\n",
    "\n",
    "### Transforming + visualizing\n",
    "\n",
    "Now, we will make dense matrices for three of our VSMs so far (raw, tf-idf, ppmi). We will:\n",
    "1. Transform matrices to have n_topics as rows instead of n_lemmas_in_vocab\n",
    "2. Reduce the matrices to 2d with umap to visualize the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecd803a286931ea80e4c8225780d13a8",
     "grade": false,
     "grade_id": "cell-06bea89519cb0f67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn\n",
    "import umap\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "n_topics = 50\n",
    "\n",
    "# making latent topics\n",
    "td_svd=TruncatedSVD(n_components=n_topics)\n",
    "td_matrix_dense = td_svd.fit_transform(td_matrix.T).T\n",
    "# 2d transformation for visualization\n",
    "td_umap = umap.UMAP(n_neighbors=10, n_components=2)\n",
    "td_matrix_umap = td_umap.fit_transform(td_matrix_dense.T).T\n",
    "\n",
    "# making latent topics\n",
    "tf_idf_svd=TruncatedSVD(n_components=n_topics)\n",
    "tf_idf_matrix_dense = tf_idf_svd.fit_transform(td_matrix.T).T\n",
    "# 2d transformation for visualization\n",
    "tf_idf_umap = umap.UMAP(n_neighbors=10, n_components=2)\n",
    "tf_idf_matrix_umap = tf_idf_umap.fit_transform(tf_idf_matrix_dense.T).T\n",
    "\n",
    "# making latent topics\n",
    "ppmi_svd=TruncatedSVD(n_components=n_topics)\n",
    "ppmi_matrix_dense = ppmi_svd.fit_transform(td_matrix.T).T\n",
    "# 2d transformation for visualization\n",
    "ppmi_umap = umap.UMAP(n_neighbors=10, n_components=2)\n",
    "ppmi_matrix_umap = ppmi_umap.fit_transform(ppmi_matrix_dense.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1be4e50417f03f464a9829d0f8c873fc",
     "grade": false,
     "grade_id": "cell-20027e4490c61b23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's visualize our models! \n",
    "When we have 2-D representations of songs, we can actually plot them and see if they make sense. Remember, we want songs by the same artist be close to each other. Can you predict which model out of these 3 will perform better? Which artist would be easier to recognize correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2dbb65d2a8c2e725453508b4e6cba92",
     "grade": false,
     "grade_id": "cell-a043812a6145d6c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_songs(song_matrix, title, songs_index):\n",
    "    \"\"\" Plots 2d vectors of songs and marks song with an artist label\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    song_matrix : numpy array\n",
    "        columns are songs\n",
    "        rows a latent topics\n",
    "    title : str\n",
    "        title for the plot\n",
    "    songs_index : dict\n",
    "        dictionary of artist and their songs indices in the song_matrix\n",
    "    indices_to_remove : list\n",
    "        list of songs to not plot\n",
    "    \"\"\"\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    pulp_indices = songs_index['pulp']\n",
    "    princess_nokia_indices = songs_index['princess_nokia']\n",
    "    at_the_drive_in_indices = songs_index['at_the_drive_in']\n",
    "    \n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "\n",
    "    pulp = plt.scatter(song_matrix[0,pulp_indices], song_matrix[1,pulp_indices], marker=\"x\", color=\"blue\")\n",
    "    princess_nokia = plt.scatter(song_matrix[0,princess_nokia_indices], song_matrix[1,princess_nokia_indices], marker=\"o\", color=\"red\")\n",
    "    at_the_drive_in = plt.scatter(song_matrix[0,at_the_drive_in_indices], song_matrix[1,at_the_drive_in_indices], marker=\"^\", color=\"black\")\n",
    "    \n",
    "    plt.legend((pulp, princess_nokia, at_the_drive_in),('Pulp','Princess Nokia','At the Drive In'))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_songs(td_matrix_umap, \"Songs as 2-D vectors (raw counts)\", normalized_songs_index_train)\n",
    "plot_songs(tf_idf_matrix_umap, \"Songs as 2-D vectors (tf-idf)\", normalized_songs_index_train)\n",
    "plot_songs(ppmi_matrix_umap, \"Songs as 2-D vectors (PPMI)\", normalized_songs_index_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27ae123e5fc9b44c680339293c7bf130",
     "grade": false,
     "grade_id": "cell-3bf052727116e4b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 4 <a class=\"anchor\" id=\"task_4\"></a>\n",
    "## Place queries into the right vector space\n",
    "\n",
    "Now we have new songs coming into our system and we would like to know what artist they were written by.\n",
    "The first thing to do is to adjust our query documents to the way we represent the songs in our systems.\n",
    "\n",
    "1. Raw counts: pre-process\n",
    "2. Collect counts\n",
    "3. Tf-idf: pre-process, collect counts, weigh\n",
    "4. PPMI: pre-process, collect counts, weigh\n",
    "5. LSI and UMAP of Raw counts: pre-process, collect counts, make dense vectors, UMAP transform\n",
    "6. LSI and UMAP of Tf-idf: pre-process, collect counts, weigh, make dense vectors, UMAP transform\n",
    "7. LSI and UMAP of PPMI: pre-process, collect counts, weigh, make dense vectors, UMAP transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ecb06897a72a1913e9879e69f1960f3e",
     "grade": false,
     "grade_id": "cell-00f4b36ee58a55ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 1: Read and Normalize songs\n",
    "the first step would be to just collect the lemmas for songs in test set using the same pre-processing pipeline as we were using for the training set. Run the cell below to collect the statistics. Make sure you haven't used the name of the stopword list, to not get different results.\n",
    "\n",
    "* `normalized_songs_test` - stores the pre-processed song from the test set\n",
    "* `normalized_songs_index_test` - record which artist the test songs belong to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "822c3c33ce33d67b9de3a794c54a9920",
     "grade": false,
     "grade_id": "cell-5792cf1ed1b62a8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collect the paths to the albums and songs\n",
    "test_songs = glob.glob('/coursedata/03-vsms/songs_test/*/*')\n",
    "\n",
    "normalized_songs_index_test  = {'pulp':None, 'princess_nokia':None, 'at_the_drive_in':None}\n",
    "normalized_songs_test = []\n",
    "\n",
    "for i, song_path in enumerate(test_songs):\n",
    "    \n",
    "    normalized_songs_test.append(tokenize_and_normalize(song_path, stop_words_english))\n",
    "    \n",
    "    if 'pulp' in song_path:\n",
    "        normalized_songs_index_test = add_song(normalized_songs_index_test,'pulp', i)\n",
    "    \n",
    "    if 'princess_nokia' in song_path:\n",
    "        normalized_songs_index_test = add_song(normalized_songs_index_test,'princess_nokia',i)\n",
    "     \n",
    "    if 'at_the_drive_in' in song_path:\n",
    "        normalized_songs_index_test = add_song(normalized_songs_index_test,'at_the_drive_in',i)\n",
    "        \n",
    "        \n",
    "# some tests to see that things are as they should         \n",
    "assert_equal(len(normalized_songs_test), 34)\n",
    "assert_equal(len(normalized_songs_index_test['pulp']), 14)\n",
    "assert_equal(len(normalized_songs_index_test['princess_nokia']), 9)\n",
    "assert_equal(len(normalized_songs_test[0]), 154)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36767478f9833391786736a933851be7",
     "grade": false,
     "grade_id": "cell-a82aeec9edbeda4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4.1 <a class=\"anchor\" id=\"#subtask_4_1=\"></a>\n",
    "### Step 2: Term-document matrix for queries (3 points)\n",
    "\n",
    "Well, this part is a bit tricky. There might be new, unseen words in our queries, and if we count them, we just won't be able to compare our new documents to our old documents (different words = different dimensions = different vector spaces). To avoid this problem, we will need to only count the things we've seen in the training corpus. Luckily, we've collected the sorted vocabulary!\n",
    "\n",
    "Modify the `create_term_doc_matrix()` function, so that it takes a query document, an old sorted vocabulary, and counts only the words that are in our train vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "086264dfd79208e69401d2533af9fbcb",
     "grade": false,
     "grade_id": "cell-34b529f25aacbca7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_term_doc_matrix_queries(normalized_queries, sorted_vocabulary):\n",
    "    \"\"\" Constructs a frequency term-document matrix for queries\n",
    "    \n",
    "    this function takes in a list of songs and a vocabulary list and returns a term-document matrix\n",
    "    the rows are lemma types as given in vocabulary, the columns are songs \n",
    "    the rows should be in the same order as in vocabulary given\n",
    "    the order of the columns should be preserved as it's given in normalized_queries\n",
    "    the cell values are a number of times a lemma was seen in a song\n",
    "    the value should be zero, if a lemma is absent from a song\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    normalized_queries : a list of lists of strings [['a','a','b','d'], ['a','b','c']]\n",
    "        a list of songs represented as a list of lemmas\n",
    "    sorted_vocabulary : list of strings\n",
    "        a list of all the lemma types used in all training songs (the rows of our matrix)\n",
    "        the words are strings sorted alphabetically\n",
    "        for our example it will be ['a','b','c']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    query_matrix : numpy array\n",
    "        a matrix where columns are songs in normalized_queries \n",
    "        and rows are lemma types from sorted_vocabulary.\n",
    "        for the example above it will be:\n",
    "            np.array([[2,1],\n",
    "                      [1,1],\n",
    "                      [0,1]])\n",
    "        'd' is not included in the matrix, becaus it is absent from sorted_vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return query_matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d359cb371fbc1a07c3cd4118ca3d7d8",
     "grade": true,
     "grade_id": "cell-b551ba0ab4efc799",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_dummy_vocab = ['!', ',', 'bla', 'la', 'oh', 'tadada', 'tarara', 'uuuuuh', 'woo']\n",
    "new_dummy_songs = [['la','oh',',','pada',\"uuuuuh\"],\n",
    "                   ['toot','toot']]\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(create_term_doc_matrix_queries(new_dummy_songs, sorted_dummy_vocab).shape, (9, 2))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_td_new_dummy_matrix = np.array([[0., 3.],\n",
    "                                        [1., 0.],\n",
    "                                        [0., 2.],\n",
    "                                        [3., 3.],\n",
    "                                        [1., 1.],\n",
    "                                        [0., 1.],\n",
    "                                        [0., 1.],\n",
    "                                        [1., 0.],\n",
    "                                        [1., 0.]])\n",
    "\n",
    "assert_array_equal(create_term_doc_matrix_queries(new_dummy_songs, sorted_dummy_vocab),np.array([[0., 0.],\n",
    "                                                                                                [1., 0.],\n",
    "                                                                                                [0., 0.],\n",
    "                                                                                                [1., 0.],\n",
    "                                                                                                [1., 0.],\n",
    "                                                                                                [0., 0.],\n",
    "                                                                                                [0., 0.],\n",
    "                                                                                                [1., 0.],\n",
    "                                                                                                [0., 0.]]))\n",
    "# SANITY CHECK FOR SONG DATA\n",
    "# check the shape\n",
    "assert_equal(create_term_doc_matrix_queries(normalized_songs_test, sorted_vocabulary).shape, \n",
    "       (len(sorted_vocabulary), len(normalized_songs_test)))\n",
    "# check the X_test[0][0] value\n",
    "assert_equal(create_term_doc_matrix_queries(normalized_songs_test, sorted_vocabulary)[0][0],0)\n",
    "# check the X_test[0][-3] value\n",
    "assert_equal(create_term_doc_matrix_queries(normalized_songs_test, sorted_vocabulary)[0][6],3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94721d535ff0d30ade2af5b49f558786",
     "grade": false,
     "grade_id": "cell-b310484643910687",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 3: Get tf-idf counts\n",
    "### Step 4: Get PPMI counts\n",
    "### Steps 5-7 transform matrices with LSI and UMAP\n",
    "Run the cell below to:\n",
    "* collect term-document matrix for the test set\n",
    "* weigh term-document matrix with tf-idf (with the help of `idf_vector` we've computed before)\n",
    "* weigh term-document matrix with PPMI\n",
    "* transform three matrices above into dense vector spaces with LSI (with the help of reducers we've fit previosly)\n",
    "* get 2d approximations with UMAP (with the help of reducers we've fit previosly)\n",
    "* plot dense vectors for songs in the test set. Are different artists separated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "571337adb4cb02f455ec60c3ba27b69f",
     "grade": false,
     "grade_id": "cell-722616992bbe0e77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# step 2 collect term-document matrix\n",
    "td_queries = create_term_doc_matrix_queries(normalized_songs_test, sorted_vocabulary)\n",
    "# step 3 weigh term-document matrix with tf-idf\n",
    "tf_idf_queries = td_queries*idf_vector \n",
    "# step 4 weigh term-document matrix with PPMI\n",
    "ppmi_queries = ppmi(td_matrix, td_queries)\n",
    "\n",
    "# steps 5-7\n",
    "td_queries_dense = td_svd.transform(td_queries.T).T\n",
    "td_queries_umap = td_umap.transform(td_queries_dense.T).T\n",
    "tf_idf_queries_dense = tf_idf_svd.transform(tf_idf_queries.T).T\n",
    "tf_idf_queries_umap = tf_idf_umap.transform(tf_idf_queries_dense.T).T\n",
    "ppmi_queries_dense = ppmi_svd.transform(ppmi_queries.T).T\n",
    "ppmi_queries_umap = ppmi_umap.transform(ppmi_queries_dense.T).T\n",
    "\n",
    "plot_songs(td_queries_umap, \"Songs as 2-D vectors (raw counts)\", normalized_songs_index_test)\n",
    "plot_songs(tf_idf_queries_umap, \"Songs as 2-D vectors (tf-idf)\", normalized_songs_index_test)\n",
    "plot_songs(ppmi_queries_umap, \"Songs as 2-D vectors (PPMI)\", normalized_songs_index_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98800c111f9e23542ba7c4646f900db7",
     "grade": false,
     "grade_id": "cell-64df99fcfb81540f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Cosine similarity\n",
    "Now that we have the documents represented as vectors, we need some measure to be able to compare them to each other. You're going to use **Cosine similarity**. You can find a more in-depth discussion of why it works in the part 6.4 of this of Jurafsky and Martin book [chapter](https://web.stanford.edu/~jurafsky/slp3/6.pdf). \n",
    "\n",
    "But the simple intuition behind it can be understood from its formula:\n",
    "\n",
    "**Cosine similarity**:\n",
    "\n",
    "$cos(x,y)=\\frac{\\sum_{i=1}^{n}x_iy_i}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\sqrt{\\sum_{i=1}^{n}y_i^2}}$\n",
    "\n",
    "As you can see, the sum in the numerater gets larger the more large values (or small negative values) two vectors have at the same dimension (same word or topic!). The disciminator helps to make two documents of different length comparable (in the case of models with raw counts, the values along different dimensions are larger for longer documents, becasue they just have more words in them). Cosine similarity rectifies it by normalizing the dot product in the numerator by vector lengths.\n",
    "\n",
    "The cell below shows an example of how to compute cosine similarities using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c43498a7d827d014d67e105e8bea06ea",
     "grade": false,
     "grade_id": "cell-829f11c7c12ffb0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "x = np.array([2,2])\n",
    "y = np.array([2,2])\n",
    "z = np.array([1,2])\n",
    "cos_sim = cosine_similarity([x],[y,z])\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeddb7b92a7c4ea7090b47cdfaff8de3",
     "grade": false,
     "grade_id": "cell-aedd17a6f54fba0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 5 <a class=\"anchor\" id=\"task_5\"></a>\n",
    "### Quality evaluation\n",
    "\n",
    "We have 6 systems now: raw counts, tf-idf, PPMI, and LSI (+UMAP for visualization) applied to the first three systems.\n",
    "\n",
    "Let's see which of them works best. We're are going to use as queries previously unseen songs. For every song in our test songs we're going to look at n closest songs from our collection and see how many of those are from the same artist. Then we are going to calculate **Precision**, **Recall**, **Accuracy**, **Error** and **F-measure** to see which system works best based on those.\n",
    "\n",
    "## 5.1 <a class=\"anchor\" id=\"subtask_5_1\"></a>\n",
    "### Find N closest documents to a query song (1 point)\n",
    "Write a function that compares new songs to the songs in our collection. For each query song, it should give out an index of top N most similar songs from our collection based on their cosine similarity. \n",
    "When a vector in a query has only zeros (it doesn't have any words appearing in training data), just output first N songs from the matrix_collection:\n",
    "1. the closest document for a zero vector has index 0\n",
    "2. the second closest document for a zero vector has index 1 and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "494a919156db10ccb956a5f612b6998e",
     "grade": false,
     "grade_id": "cell-1dad28ebe8df8527",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def closest_n_documents(matrix_collection, matrix_queries, n):\n",
    "    \"\"\"Finds N closest documents from a training collection to every song in a test collection\n",
    "    \n",
    "    this function takes in original document collection, new document collection,\n",
    "    computes cosine similarity between documents in old and new collection, \n",
    "    and outputs the list of n-closest documents to each new song\n",
    "    when a vector in a query has only zeros, \n",
    "        the closeness to it should be determined by index of a song from a matrix_collection:\n",
    "        the closest document for a zero vector has index 0\n",
    "        the second closest document for a zero vector has index 1 and so on\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_collection : numpy array\n",
    "        a term-document matrix of songs in training collection\n",
    "        songs are columns\n",
    "    matrix_queries : numpy array\n",
    "        a term-document matrix of query songs\n",
    "        songs are columns\n",
    "    n : int\n",
    "        a number of closest documents to return\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    closest_docs : a list of lists \n",
    "        a list of length equal to the number of songs in a query matrix\n",
    "        each element is, in turn, a list of n indices of documents in matrix_collection that were closest to the query\n",
    "        for n=2 and query matrix with 3 songs, the output should look like so [[1,2],[1,2],[1,2]]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return best_cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "715c3085e99b60cf538b987bf7340cc8",
     "grade": true,
     "grade_id": "cell-0cbe6d8fd7344b44",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal, assert_array_equal, assert_allclose\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "\n",
    "dummy_collection = np.concatenate((np.arange(15).reshape((5,3)),np.zeros((5,1))), axis=1)\n",
    "dummy_query_1 = np.arange(5).reshape((5,1))\n",
    "dummy_query_2 = np.arange(10).reshape((5,2))\n",
    "dummy_query_3 = np.arange(15).reshape((5,3))\n",
    "dummy_query_4 = np.zeros((5,2))\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the length of the list\n",
    "assert_equal(len(closest_n_documents(dummy_collection, dummy_query_1, 1)), 1)\n",
    "assert_equal(len(closest_n_documents(dummy_collection, dummy_query_2, 1)), 2)\n",
    "# check the len of the first element\n",
    "assert_equal(len(closest_n_documents(dummy_collection, dummy_query_1, 1)[0]), 1)\n",
    "assert_equal(len(closest_n_documents(dummy_collection, dummy_query_2, 3)[0]), 3)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "closest_vector_id_1 = closest_n_documents(dummy_collection, dummy_query_1, 1)[0][0]\n",
    "assert_equal(closest_vector_id_1, 0)\n",
    "\n",
    "closest_vector_id_2 = closest_n_documents(dummy_collection, dummy_query_1, 4)\n",
    "assert_equal(closest_vector_id_2, [[0, 1, 2, 3]])\n",
    "\n",
    "closest_vector_id_3 = closest_n_documents(dummy_collection, dummy_query_3, 1)\n",
    "assert_equal(closest_vector_id_3, [[0],[1],[2]])\n",
    "\n",
    "closest_vector_id_4 = closest_n_documents(dummy_collection, dummy_query_4, 2)\n",
    "assert_equal(closest_vector_id_4, [[0, 1], [0, 1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "875966d55c09b7ab682f81a4a1a5b3f1",
     "grade": false,
     "grade_id": "cell-11622188becf085e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Collect performance statistics for every query song by every model\n",
    "For every model out of 6 models we have, the code below collects the closest 7 songs from a train collection for every song from a test collection and stores this informatiob into `closest_songs` array.\n",
    "\n",
    "So, `closest_songs` has 6 lists (for six models). Each list is a list of L lists (L is the number of songs in the test set). Every list here is a list of 7 songs from the train collection that a model thinks is the most similar to a test song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf96a61f16b88e4221d444e14c654263",
     "grade": false,
     "grade_id": "cell-c7018097db516c6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [td_matrix, tf_idf_matrix, ppmi_matrix, \n",
    "          td_matrix_dense, tf_idf_matrix_dense, ppmi_matrix_dense]\n",
    "queries = [td_queries, tf_idf_queries, ppmi_queries,\n",
    "           td_queries_dense, tf_idf_queries_dense, ppmi_queries_dense]\n",
    "\n",
    "closest_songs = [] # a list of closest songs for every song in every VSM model \n",
    "\n",
    "for i in range(len(models)):\n",
    "    closest_songs.append(closest_n_documents(models[i], queries[i], 7))\n",
    "    \n",
    "print(\"There are results for {} models.\".format(len(closest_songs)))\n",
    "print(\"Every model has results for {} songs.\".format(len(closest_songs[0])))\n",
    "print(\"Every song has been assigned {} closest songs.\".format(len(closest_songs[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "297257a4f5f207bfd3299ff8b2201970",
     "grade": false,
     "grade_id": "cell-4c1f7140f72388a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5.2  <a class=\"anchor\" id=\"subtask_5_2\"></a>\n",
    "### Compare models (4 points)\n",
    "\n",
    "Now we've collected \\[closest 7 songs out of training corpus\\] for \\[each model and for each song in our test corpus\\] let's evaluate our models. We will be looking at five measures: \n",
    "\n",
    "1. Precision = $\\frac{tp}{tp + fp}$ (how many songs by the right artist from all 7 similar songs)\n",
    "2. Recall = $\\frac{tp}{tp + fn}$ (how many songs by the right artist from all their possible songs)\n",
    "3. Accuracy = $\\frac{tp+tn}{N}$ (how many songs by the right artist were chosen and how many songs by the wrong artist we NOT chosen, out of all songs in collection)\n",
    "4. Error = $\\frac{fp+fn}{N}$ (how many songs by the right artist were NOT chosen, and how many songs by the wrong artist were chosen, out of all songs in collection)\n",
    "5. F-measure $\\frac{1}{\\alpha\\frac{1}{P}+(1-\\alpha)\\frac{1}{R}}$\n",
    "$P$ stands for precision and $R$ for recall, while $\\alpha$ controls the weighting between them. If we choose $\\alpha = 0.5$, $F = \\frac{2PR}{P+R}$\n",
    "\n",
    "\n",
    "Here: $tp$ = True Positives, $fp$ = False Positives, $fn$ = False Negatives, $tn$ = True Negatives, $N$ = the size of the collection.\n",
    "\n",
    "Let's unpack those a bit further. Let's say we have a collection of two artists A and B. The songs by A have indices \\[1,2,3\\], the songs by B have indices \\[4,5,6,7\\]. Now we have a new song coming from artist B, and we look for 3 closest songs to it and find \\[1,5,4\\]\n",
    "\n",
    "* True Positive - an index out of n closest songs belonging to a song by the same artist as a query (5 and 4 in our example)\n",
    "* False Positive - an index out of n closest songs belonging to a song by an artist different from a query (1 in our example)\n",
    "* False Negative - and index of a song by the same artist as a query that was (wrongly) not in the n best list (6 and 7 in our example)\n",
    "* True Negative - an index of a song by a different artist that was (rightly) not in the n best list (2 and 3 in out example)\n",
    "* N - the number of all songs in the collection (7 in our example)\n",
    "\n",
    "SO:\n",
    "* Precision = $\\frac{2}{3}$\n",
    "* Recall = $\\frac{2}{4}$\n",
    "* Accuracy = $\\frac{4}{7}$\n",
    "* Error = $\\frac{3}{7}$\n",
    "* F-measure = $\\frac{2*0.67*0.5}{1.17}$, (with alpha being 0.5)\n",
    "\n",
    "\n",
    "**Note that these metrics depend on the number of closests songs we want to get!** If we asked for 2 closests songs, the calculations would differ. What metric will have more chances of being higher with higher number of closests songs?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 5.2.1 <a class=\"anchor\" id=\"subsubtask_5_2_1\"></a>\n",
    "### Count Negatives and Positives (3 points)\n",
    "\n",
    "In the cell below, create a function that counts the number TP, FP, FN, TP for a song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "454db0ac30a28f380132dd8538a702b4",
     "grade": false,
     "grade_id": "cell-f7595a21186b1afe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_results(song_closest_songs, song_artist, train_index):\n",
    "    \"\"\"Counts TP, FP, FN, and TN for a query song results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    song_closest_songs : a list of ints\n",
    "        a list of indices of n closest songs from a training collection\n",
    "    song_artist : str\n",
    "        the name of the atrist of the song tested (key to the index dictionary)\n",
    "    train_index : dict {atrist:lits of song indices}\n",
    "        indices of songs in training collection assigned to artists\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    song_results : a list of ints\n",
    "        [TP, FP, FN, TN]\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    song_results = [TP, FP, FN, TN]\n",
    "    return song_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0be143cff577accff0501d919e5294f9",
     "grade": true,
     "grade_id": "cell-c7dc4d52eced20fb",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_train_index = {'A':[1,2,3],'B':[4,5,6,7]}\n",
    "dummy_test_index = {'B':[0,2], 'A':[1,3]}\n",
    "# four songs\n",
    "# partial match for the \n",
    "# no matches for the second song\n",
    "# perfect match for the third song\n",
    "# perfect match for the 4th song\n",
    "dummy_closest_songs =[[1,5,4],[4,5,6],[4,5,6],[1,2,3]]\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check if function outputs a list\n",
    "assert_equal(type(count_results(dummy_closest_songs[0], 'B', dummy_train_index)), list)\n",
    "# check that function returns a list of all ints\n",
    "assert(all([type(i)==int for i in count_results(dummy_closest_songs[0], 'B', dummy_train_index)]))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "assert_equal(count_results(dummy_closest_songs[0], 'B', dummy_train_index), [2,1,2,2])\n",
    "assert_equal(count_results(dummy_closest_songs[1], 'A', dummy_train_index), [0,3,3,1])\n",
    "assert_equal(count_results(dummy_closest_songs[2], 'B', dummy_train_index), [3,0,1,3])\n",
    "assert_equal(count_results(dummy_closest_songs[3], 'A', dummy_train_index), [3,0,0,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59b4924f166ed565308a9caebb538e82",
     "grade": false,
     "grade_id": "cell-9f7f48276d971493",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5.2.2 <a class=\"anchor\" id=\"subsubtask_5_2_2\"></a>\n",
    "### Compute the metrics (1 point)\n",
    "\n",
    "Now that we have our results, let's compute the average metrics for each model.\n",
    "\n",
    "Average model scores for each of these 6 metrics. That is: compute the metrics, sum them and divide by the number of songs in the test set. For the F-measure choose $\\alpha = 0.5$. In case you have no True Positives set F-measure to zero to avoid 'division by zero' error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9b11ee03e1a08a2fa6b8ddc71c2762f",
     "grade": false,
     "grade_id": "cell-1b165b9b52c73bbf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(test_song_results, N):\n",
    "    \"\"\"Computes metrics for a song based on the number of relevant songs in its n closest songs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_song_results : a list of lists of ints\n",
    "        positives and negatives for every song in a test set\n",
    "        [[TP, FP, FN, TN], [TP, FP, FN, TN]]\n",
    "    N : int\n",
    "        the number of the songs in our train collection\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    average_precision: float \n",
    "        TP/(TP+FP)\n",
    "    average_recall: float\n",
    "        TP/(FN+TP)\n",
    "    average_accuracy: float\n",
    "        (TP+TN)/N\n",
    "    average_error: float\n",
    "        (FP+FN)/N\n",
    "    average_f_measure: float\n",
    "        2*precision*recall/(precision+recall)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return average_precision, average_recall, average_accuracy, average_error, average_f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84e4855fc9308c6bedad8ad783246636",
     "grade": true,
     "grade_id": "cell-2db281724fb08e03",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_train_index = {'A':[1,2,3],'B':[4,5,6,7]}\n",
    "dummy_test_index = {'B':[0,2], 'A':[1,3]}\n",
    "# four songs\n",
    "# partial match for the \n",
    "# no matches for the second song\n",
    "# perfect match for the third song\n",
    "# perfect match for the 4th song\n",
    "dummy_closest_songs =[[1,5,4],[4,5,6],[4,5,6],[1,2,3]]\n",
    "dummy_songs_results =[[2,1,2,2], [0,3,3,1], [3,0,1,3],[3,0,0,4]]\n",
    "\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check if function outputs 5 metrics\n",
    "assert_equal(len(compute_metrics([dummy_songs_results[0]],7)), 5)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# checking that our first dummy example is correct\n",
    "# precision\n",
    "assert_almost_equal(compute_metrics([dummy_songs_results[0]],7)[0], 0.6666666666666666,2)\n",
    "# recall\n",
    "assert_almost_equal(compute_metrics([dummy_songs_results[0]],7)[1], 0.5,2)\n",
    "# accuracy\n",
    "assert_almost_equal(compute_metrics([dummy_songs_results[0]],7)[2], 0.5714285714285714,2)\n",
    "# error\n",
    "assert_almost_equal(compute_metrics([dummy_songs_results[0]],7)[3], 0.42857142857142855,2)\n",
    "# f measure\n",
    "assert_almost_equal(compute_metrics([dummy_songs_results[0]],7)[4], 0.5714285714285715,2)\n",
    "\n",
    "\n",
    "# checking that the average metris of our dummy example are correct\n",
    "\n",
    "assert_almost_equal(compute_metrics(dummy_songs_results,7)[0], 0.6666666666666666,2)\n",
    "# recall\n",
    "assert_almost_equal(compute_metrics(dummy_songs_results,7)[1], 0.5625,2)\n",
    "# accuracy\n",
    "assert_almost_equal(compute_metrics(dummy_songs_results,7)[2], 0.6428571428571428,2)\n",
    "# error\n",
    "assert_almost_equal(compute_metrics(dummy_songs_results,7)[3], 0.3571428571428571,2)\n",
    "# f measure\n",
    "assert_almost_equal(compute_metrics(dummy_songs_results,7)[4], 0.6071428571428572,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f78d36da93da059316dbb7748bb6273",
     "grade": false,
     "grade_id": "cell-dff0d488a6969696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's take a look at the results!!!!!\n",
    "Run the cell below to compute the results for every VSM model we've built and put them in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "827b7a7fe12ffb2d9cffc3dc07cbc9bb",
     "grade": false,
     "grade_id": "cell-688800942fe8b15b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns =['Precision', 'Recall', 'Accuracy', 'Error', 'F_measure']\n",
    "rows = ['Raw counts', 'tf-ifd', 'PPMI', 'Raw counts dense', 'tf-ifd dense', 'PPMI dense']\n",
    "df = pd.DataFrame(columns=columns , index=rows)\n",
    "\n",
    "\n",
    "N=sum([len(v) for v in normalized_songs_index_train.values()])\n",
    "for i, song_results in enumerate(closest_songs): # for model\n",
    "    model_results = []\n",
    "    for artist in normalized_songs_index_test.keys():\n",
    "        for song_id in normalized_songs_index_test[artist]:\n",
    "            song_closest_songs = closest_songs[i][song_id]\n",
    "            model_results.append(count_results(song_closest_songs, artist, normalized_songs_index_train))\n",
    "                                 \n",
    "    model_metrics = compute_metrics(model_results, N)\n",
    "\n",
    "    \n",
    "    \n",
    "    df.loc[rows[i]] = list(model_metrics)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17bdb8ffb44af3008c35497773d907a0",
     "grade": false,
     "grade_id": "cell-88cdc5de83c20c80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's take a look at different artists\n",
    "Run the cell below to get results for each artist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9b1876dc5f79f53a87600cc86fea0c4",
     "grade": false,
     "grade_id": "cell-64c5f5a5826afac6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns =['Precision', 'Recall', 'Accuracy', 'Error', 'F_measure']\n",
    "rows = ['Raw counts', 'tf-ifd', 'PPMI', 'Raw counts dense', 'tf-ifd dense', 'PPMI dense']\n",
    "artists = ['pulp', 'princess_nokia', 'at_the_drive_in']\n",
    "\n",
    "new_columns = []\n",
    "for artist in artists:\n",
    "    for column in columns:\n",
    "            new_columns.append(column+'_'+artist)\n",
    "\n",
    "df = pd.DataFrame(columns=rows , index=new_columns)\n",
    "\n",
    "    \n",
    "N=sum([len(v) for v in normalized_songs_index_train.values()])\n",
    "\n",
    "for i, song_results in enumerate(closest_songs): # for model\n",
    "    model_results = []\n",
    "    for artist in normalized_songs_index_test.keys(): # for artist\n",
    "        artist_song_results=[]\n",
    "        for song_id in normalized_songs_index_test[artist]:\n",
    "            song_closest_songs = closest_songs[i][song_id]\n",
    "            artist_song_results.append(count_results(song_closest_songs, artist, normalized_songs_index_train))\n",
    "        artrist_metrics = compute_metrics(artist_song_results, N)\n",
    "        model_results+=list(artrist_metrics)\n",
    "    df[rows[i]] = model_results\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55024820cb091db1ac560c9830075bd5",
     "grade": false,
     "grade_id": "cell-ac1d2fe8616c0d95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 7.3 <a class=\"anchor\" id=\"subtask_7_3\"></a>\n",
    "### Choose the best model (3 points)\n",
    "After looking at the result tables, answer briefly in the cell below: \n",
    "* What model/models performed best? (0.5 points)\n",
    "* What do you think the reason is? Whould dense results differ if we used less/more dimensions (you can try out 3/300 topics vs our 50 topics)? (1 point)\n",
    "* What metrics helped you to form this opinion and why? (Was there a metric you didn't use? Why?) (0.5 point)\n",
    "* What artist was the easiest to get right? Why do you think this happend? (Can it be the number of word types/tokens in their songs? Any vocabulary overlaps between artists?) (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f25509dead647432e8e897dd249361be",
     "grade": false,
     "grade_id": "cell-31d6bb5e94c1bc20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checklist before submission <a class=\"anchor\" id=\"checklist\"></a>\n",
    "### 1\n",
    "To make sure that you didn't forget to import some package or to name some variable, press **Kernel -> Restart** and then **Cell -> Run All**. This way your code will be run exactly in the same order as during the autograding.\n",
    "### 2\n",
    "Click the **Validate** button in the upper menu to check that you haven't missed anything. Be careful now (our text pre-processing takes time).\n",
    "### 3\n",
    "To submit the notebook, click on the **jupyterhub** logo in the upper left part of the window, choose the **Assignments** folder, and press **submit**. You can submit multiple times, only the last one counts.\n",
    "### 4\n",
    "Please fill in the feedback form in the [Assignment](https://mycourses.aalto.fi/mod/questionnaire/view.php?id=1122380) section of Mycoures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
